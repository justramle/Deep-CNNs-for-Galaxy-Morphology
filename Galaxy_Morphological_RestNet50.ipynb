{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###################  GALAXY GEM ###############################\n",
        "######## Project Topic: Using Convolutional Neural Networks for Galaxy Morphology Classification ########\n",
        "#######  Group Members:\n",
        "#######  Tram Le – hle12@kent.edu\n",
        "#######  Thi Minh Thu Nguyen – tnguye69@kent.edu\n",
        "#######  Nickson Ibrahim Makama – nibrahi3@kent.edu\n",
        "#######  Thanyaporn Noiplab – tnoiplab@kent.edu\n",
        "###########################################################"
      ],
      "metadata": {
        "id": "mKeuWSEkV0wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRA7TbdkL5aC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58318d32-c32c-4543-b86f-8b1fe92327f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting astroNN\n",
            "  Downloading astroNN-1.1.0-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from astroNN) (1.25.2)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (from astroNN) (5.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from astroNN) (3.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from astroNN) (3.7.1)\n",
            "Collecting astroquery (from astroNN)\n",
            "  Downloading astroquery-0.4.7-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from astroNN) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from astroNN) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from astroNN) (4.66.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from astroNN) (24.0)\n",
            "Requirement already satisfied: tensorflow>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from astroNN) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from astroNN) (0.23.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->astroNN) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.19.0->astroNN) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.19.0->astroNN) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.19.0->astroNN) (0.1.8)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy->astroNN) (2.0.1.4)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy->astroNN) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.10/dist-packages (from astroquery->astroNN) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.8 in /usr/local/lib/python3.10/dist-packages (from astroquery->astroNN) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.10/dist-packages (from astroquery->astroNN) (1.1)\n",
            "Requirement already satisfied: keyring>=15.0 in /usr/lib/python3/dist-packages (from astroquery->astroNN) (23.5.0)\n",
            "Collecting pyvo>=1.1 (from astroquery->astroNN)\n",
            "  Downloading pyvo-1.5.1-py3-none-any.whl (910 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.2/910.2 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->astroNN) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->astroNN) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->astroNN) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->astroNN) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->astroNN) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->astroNN) (3.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.11.0->astroNN) (0.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.8->astroquery->astroNN) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=0.999->astroquery->astroNN) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->astroquery->astroNN) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->astroquery->astroNN) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->astroquery->astroNN) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->astroquery->astroNN) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.11.0->astroNN) (3.2.2)\n",
            "Installing collected packages: pyvo, astroquery, astroNN\n",
            "Successfully installed astroNN-1.1.0 astroquery-0.4.7 pyvo-1.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install astroNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92TtOnpnQ9B-",
        "outputId": "a6abc49e-cac7-4dfe-bde3-d12feffa1953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/841.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/841.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTc-EQEslRAn"
      },
      "outputs": [],
      "source": [
        "import imp\n",
        "from astroNN.datasets import load_galaxy10\n",
        "from tensorflow.keras import utils\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch, time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchmetrics.classification import Accuracy\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH-dFfv9MMpn",
        "outputId": "f0405961-e7b3-411f-e0d5-afbb0005c057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# define device for Torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLYOFtEoM2H7",
        "outputId": "78c114d4-07eb-4cad-bfe6-efc3a9c2a652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Galaxy10_DECals.h5: 100%|█████████▉| 2.73G/2.74G [03:08<00:00, 14.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Galaxy10 successfully to /root/.astroNN/datasets/Galaxy10_DECals.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGalaxy10_DECals.h5: 2.74GB [03:16, 13.9MB/s]                            \n"
          ]
        }
      ],
      "source": [
        "images, labels = load_galaxy10()\n",
        "\n",
        "# To convert the labels to categorical 10 classes\n",
        "labels = utils.to_categorical(labels, 10)\n",
        "\n",
        "# To convert to float\n",
        "labels = labels.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zcpzk9nxNADd"
      },
      "outputs": [],
      "source": [
        "## Split the dataset 90/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O684AgyxM-zV",
        "outputId": "793a4800-399d-47cf-f901-6bd87fabb843"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15962, 256, 256, 3), (15962, 10), (1774, 256, 256, 3), (1774, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_idx, test_idx = train_test_split(np.arange(labels.shape[0]), test_size=0.2)\n",
        "train_images, train_labels, test_images, test_labels = images[train_idx], labels[train_idx], images[test_idx], labels[test_idx]\n",
        "\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kR6bGHoN_aD"
      },
      "source": [
        "### Build custom class to takes in images and labels and transform to be applied bc only to do augmentation of train set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWlrGJUNdIQ"
      },
      "outputs": [],
      "source": [
        "class Galaxy10(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img =  img.astype(np.uint8)\n",
        "            img = Image.fromarray(img)        # Ensure img is PILLOW Image cuz currently it's a numpy array\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjKZWMuaOqGY"
      },
      "source": [
        "### Do augmentation and transform the images\n",
        "- https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2uPYpo8ONUp",
        "outputId": "7bfe63d6-d19d-4bf0-a235-18f617dc04ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15962\n",
            "1774\n"
          ]
        }
      ],
      "source": [
        "# augmentation and convert to tensor for train\n",
        "transform_train = transforms.Compose([\n",
        "      transforms.TrivialAugmentWide(),\n",
        "      # transforms.RandomAffine(degrees=20, translate=(0.1, 0.1)),\n",
        "      # transforms.ColorJitter(brightness=(0.9, 2), contrast=(0.8, 1.2)),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomVerticalFlip(),\n",
        "      transforms.Resize(224),\n",
        "      transforms.ToTensor(),         # Converts np (H x W x C) to torch (C x H x W)\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))       # and norm from 0-255 to 0-1\n",
        "      ])\n",
        "\n",
        "# only convert to tensor for train\n",
        "transform_test = transforms.Compose([\n",
        "      transforms.Resize(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "      ])\n",
        "\n",
        "train_ds = Galaxy10(train_images, train_labels, transform=transform_train)\n",
        "test_ds = Galaxy10(test_images, test_labels, transform=transform_test)\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmcU2bNmTbHn"
      },
      "source": [
        "### Use DataLoader to helps to speed up computation and reduce RAM usage, handling loading and batching during training\n",
        "- Use GradScaler to help scaling the loss value, control the gradients range, faster training on NVIDIA GPUs with mix precision with both float16 and float32 precisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiWNPtbcSN-W",
        "outputId": "2bfef5e8-43f8-4055-b7a5-50b376805fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number batches in training: 250\n",
            "Number batches in test: 28\n",
            "Shape of 1st batch training (after transformations): torch.Size([64, 3, 224, 224])\n",
            "Shape of the (labels): torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# import torch.multiprocessing as mp\n",
        "# mp.set_start_method('spawn', force=True) # fix the fork issue in colab\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds,\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=0,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "# no shuffle on test loader\n",
        "test_loader = torch.utils.data.DataLoader(test_ds,\n",
        "                                         batch_size=64,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=0,\n",
        "                                         pin_memory=True)\n",
        "\n",
        "# Checking the number of batches in each loader\n",
        "print(f\"Number batches in training: {len(train_loader)}\")    # 14188 / 64  = 222 batch\n",
        "print(f\"Number batches in test: {len(test_loader)}\")\n",
        "for images, labels in train_loader:\n",
        "    print(f\"Shape of 1st batch training (after transformations): {images.shape}\")\n",
        "    print(f\"Shape of the (labels): {labels.shape}\")  # Should 128\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHDTKLDAR22y"
      },
      "source": [
        "### Build Model\n",
        "\n",
        "- freeze the weights in the Conv layers (so they wont change during training) so that only train new set of layers at the end of the network.\n",
        "- new layers will learn to make predict based on the feature maps from  \n",
        "pre-trained.\n",
        "- give a bad result from pretrain, so train model with random initilized weight from scatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_G5ygpqWx3S",
        "outputId": "afd1ac89-703d-4029-ddf6-9cee2893992e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-143          [-1, 512, 14, 14]               0\n",
            "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-146            [-1, 512, 7, 7]               0\n",
            "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-151           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-155            [-1, 512, 7, 7]               0\n",
            "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-158            [-1, 512, 7, 7]               0\n",
            "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-161           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-165            [-1, 512, 7, 7]               0\n",
            "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-168            [-1, 512, 7, 7]               0\n",
            "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-171           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "         Flatten-174                 [-1, 2048]               0\n",
            "         Dropout-175                 [-1, 2048]               0\n",
            "          Linear-176                  [-1, 512]       1,049,088\n",
            "            ReLU-177                  [-1, 512]               0\n",
            "         Dropout-178                  [-1, 512]               0\n",
            "          Linear-179                  [-1, 128]          65,664\n",
            "            ReLU-180                  [-1, 128]               0\n",
            "         Dropout-181                  [-1, 128]               0\n",
            "          Linear-182                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 24,624,074\n",
            "Trainable params: 24,624,074\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 286.60\n",
            "Params size (MB): 93.93\n",
            "Estimated Total Size (MB): 381.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_model = models.resnet50(pretrained=False)\n",
        "# # Freeze all the parameters in the feature extraction\n",
        "# for param in base_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "\n",
        "#=========== Tranfer learning 1\n",
        "class RestNet(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(RestNet, self).__init__()\n",
        "\n",
        "        # Remove the last of resnet layer\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
        "\n",
        "        # Append more FC layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 512),\n",
        "            # nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(512, 128),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(128, 10),  #FC 10 classes\n",
        "            # nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Pass input through the feature layers\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = RestNet(base_model)\n",
        "model.to(device)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fOvCB66Wyxw"
      },
      "source": [
        "### Determine:\n",
        "- The metrics: accurary to cal the number of correctly predicted results / total number of samples\n",
        "- Objective funtion for loss is crossentropy because this is classification problems\n",
        "- Optimizer with Adam\n",
        "- Add L2 regulation at 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agSHufisYzaO"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZjzSMw3Uy5W",
        "outputId": "3f1043e7-9b26-466a-eb9c-176774cbc45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adam\n"
          ]
        }
      ],
      "source": [
        "top1_acc = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
        "\n",
        "#Mixed Precision training\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "\n",
        "print(\"Training with Adam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8C1FZ83ZCrz"
      },
      "source": [
        "### Train process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPfj4NpDbQB5",
        "outputId": "d1802db6-1161-4eb9-af72-403374fe2a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directory already exists.\n",
            "Saving model to: /content/drive/My Drive/Model/galaxy_r50_model.pth\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory and the filename\n",
        "directory_path = '/content/drive/My Drive/Model'\n",
        "model_filename = \"galaxy_r50_model.pth\"\n",
        "save_path = os.path.join(directory_path, model_filename)\n",
        "\n",
        "import os\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "    print(\"Directory created:\", directory_path)\n",
        "else:\n",
        "    print(\"Directory already exists.\")\n",
        "\n",
        "print(\"Saving model to:\", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBYaSKaMcUST",
        "outputId": "f09e82b5-c125-4711-9290-f21dc8fc3126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 2.2592 - train acc: 0.1360]\n",
            "Epoch 1 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s, val loss: 2.2082 - val acc: 0.1722]\n",
            "Epoch 2 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 2.0898 - train acc: 0.2066]\n",
            "Epoch 2 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.88batch/s, val loss: 1.9170 - val acc: 0.2734]\n",
            "Epoch 3 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.9240 - train acc: 0.2749]\n",
            "Epoch 3 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.08batch/s, val loss: 1.8224 - val acc: 0.3043]\n",
            "Epoch 4 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.7459 - train acc: 0.3360]\n",
            "Epoch 4 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.07batch/s, val loss: 1.5216 - val acc: 0.4222]\n",
            "Epoch 5 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.6602 - train acc: 0.3743]\n",
            "Epoch 5 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.69batch/s, val loss: 1.4477 - val acc: 0.4465]\n",
            "Epoch 6 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.5915 - train acc: 0.4058]\n",
            "Epoch 6 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.69batch/s, val loss: 1.5046 - val acc: 0.4009]\n",
            "Epoch 7 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.5178 - train acc: 0.4396]\n",
            "Epoch 7 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.05batch/s, val loss: 1.2875 - val acc: 0.5432]\n",
            "Epoch 8 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.4866 - train acc: 0.4560]\n",
            "Epoch 8 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.09batch/s, val loss: 1.5172 - val acc: 0.4703]\n",
            "Epoch 9 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.4641 - train acc: 0.4640]\n",
            "Epoch 9 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.01batch/s, val loss: 1.2379 - val acc: 0.5534]\n",
            "Epoch 10 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.4216 - train acc: 0.4810]\n",
            "Epoch 10 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.71batch/s, val loss: 1.2461 - val acc: 0.5411]\n",
            "Epoch 11 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.4131 - train acc: 0.4889]\n",
            "Epoch 11 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.96batch/s, val loss: 1.3082 - val acc: 0.5270]\n",
            "Epoch 12 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.3924 - train acc: 0.4919]\n",
            "Epoch 12 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.03batch/s, val loss: 1.2032 - val acc: 0.5423]\n",
            "Epoch 13 Training: 100%|██████████| 250/250 [00:58<00:00,  4.25batch/s, train loss: 1.3636 - train acc: 0.5014]\n",
            "Epoch 13 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 1.2559 - val acc: 0.5348]\n",
            "Epoch 14 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.3456 - train acc: 0.5066]\n",
            "Epoch 14 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.08batch/s, val loss: 1.5204 - val acc: 0.4334]\n",
            "Epoch 15 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.3268 - train acc: 0.5210]\n",
            "Epoch 15 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.72batch/s, val loss: 1.2000 - val acc: 0.5748]\n",
            "Epoch 16 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.3190 - train acc: 0.5272]\n",
            "Epoch 16 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.05batch/s, val loss: 1.2247 - val acc: 0.5578]\n",
            "Epoch 17 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.3069 - train acc: 0.5301]\n",
            "Epoch 17 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.11batch/s, val loss: 1.2597 - val acc: 0.5456]\n",
            "Epoch 18 Training: 100%|██████████| 250/250 [00:58<00:00,  4.24batch/s, train loss: 1.2916 - train acc: 0.5447]\n",
            "Epoch 18 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.02batch/s, val loss: 1.1864 - val acc: 0.5495]\n",
            "Epoch 19 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 1.2613 - train acc: 0.5605]\n",
            "Epoch 19 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.56batch/s, val loss: 1.4802 - val acc: 0.4900]\n",
            "Epoch 20 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.2570 - train acc: 0.5613]\n",
            "Epoch 20 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.91batch/s, val loss: 1.1061 - val acc: 0.5788]\n",
            "Epoch 21 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.2160 - train acc: 0.5822]\n",
            "Epoch 21 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.01batch/s, val loss: 1.2539 - val acc: 0.5661]\n",
            "Epoch 22 Training: 100%|██████████| 250/250 [00:58<00:00,  4.24batch/s, train loss: 1.1868 - train acc: 0.5930]\n",
            "Epoch 22 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 1.1256 - val acc: 0.5950]\n",
            "Epoch 23 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.1748 - train acc: 0.6049]\n",
            "Epoch 23 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 1.0314 - val acc: 0.6559]\n",
            "Epoch 24 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.1442 - train acc: 0.6148]\n",
            "Epoch 24 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.87batch/s, val loss: 1.0491 - val acc: 0.6435]\n",
            "Epoch 25 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 1.1378 - train acc: 0.6157]\n",
            "Epoch 25 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.05batch/s, val loss: 0.9535 - val acc: 0.6751]\n",
            "Epoch 26 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.1060 - train acc: 0.6340]\n",
            "Epoch 26 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 1.0381 - val acc: 0.6607]\n",
            "Epoch 27 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.0927 - train acc: 0.6346]\n",
            "Epoch 27 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 0.9989 - val acc: 0.6724]\n",
            "Epoch 28 Training: 100%|██████████| 250/250 [00:58<00:00,  4.25batch/s, train loss: 1.0664 - train acc: 0.6480]\n",
            "Epoch 28 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.57batch/s, val loss: 1.2007 - val acc: 0.5792]\n",
            "Epoch 29 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.0532 - train acc: 0.6507]\n",
            "Epoch 29 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.84batch/s, val loss: 0.8318 - val acc: 0.7211]\n",
            "Epoch 30 Training: 100%|██████████| 250/250 [00:58<00:00,  4.24batch/s, train loss: 1.0730 - train acc: 0.6428]\n",
            "Epoch 30 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.06batch/s, val loss: 0.8815 - val acc: 0.7064]\n",
            "Epoch 31 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 1.0444 - train acc: 0.6589]\n",
            "Epoch 31 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 0.8886 - val acc: 0.7081]\n",
            "Epoch 32 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 1.0233 - train acc: 0.6614]\n",
            "Epoch 32 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.08batch/s, val loss: 0.8515 - val acc: 0.7226]\n",
            "Epoch 33 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.0118 - train acc: 0.6726]\n",
            "Epoch 33 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.71batch/s, val loss: 1.0065 - val acc: 0.6577]\n",
            "Epoch 34 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.0099 - train acc: 0.6688]\n",
            "Epoch 34 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.99batch/s, val loss: 1.0289 - val acc: 0.6695]\n",
            "Epoch 35 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.0010 - train acc: 0.6728]\n",
            "Epoch 35 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 0.8023 - val acc: 0.7492]\n",
            "Epoch 36 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.0071 - train acc: 0.6715]\n",
            "Epoch 36 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.98batch/s, val loss: 0.8404 - val acc: 0.7178]\n",
            "Epoch 37 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.9838 - train acc: 0.6816]\n",
            "Epoch 37 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.55batch/s, val loss: 0.9051 - val acc: 0.7097]\n",
            "Epoch 38 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 1.0003 - train acc: 0.6734]\n",
            "Epoch 38 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.94batch/s, val loss: 0.8902 - val acc: 0.7124]\n",
            "Epoch 39 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.9815 - train acc: 0.6855]\n",
            "Epoch 39 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.04batch/s, val loss: 0.8454 - val acc: 0.7206]\n",
            "Epoch 40 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.9677 - train acc: 0.6869]\n",
            "Epoch 40 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.01batch/s, val loss: 0.9073 - val acc: 0.7054]\n",
            "Epoch 41 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.9691 - train acc: 0.6891]\n",
            "Epoch 41 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.63batch/s, val loss: 0.8851 - val acc: 0.7039]\n",
            "Epoch 42 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.9728 - train acc: 0.6835]\n",
            "Epoch 42 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s, val loss: 0.7754 - val acc: 0.7566]\n",
            "Epoch 43 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.9595 - train acc: 0.6873]\n",
            "Epoch 43 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.06batch/s, val loss: 0.7600 - val acc: 0.7623]\n",
            "Epoch 44 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.9497 - train acc: 0.6970]\n",
            "Epoch 44 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.8308 - val acc: 0.7265]\n",
            "Epoch 45 Training: 100%|██████████| 250/250 [00:58<00:00,  4.24batch/s, train loss: 0.9492 - train acc: 0.6977]\n",
            "Epoch 45 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.8467 - val acc: 0.7326]\n",
            "Epoch 46 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.9391 - train acc: 0.7007]\n",
            "Epoch 46 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.88batch/s, val loss: 0.8974 - val acc: 0.7162]\n",
            "Epoch 47 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.9389 - train acc: 0.7008]\n",
            "Epoch 47 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.05batch/s, val loss: 0.8668 - val acc: 0.7122]\n",
            "Epoch 48 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.9398 - train acc: 0.7017]\n",
            "Epoch 48 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.99batch/s, val loss: 0.7873 - val acc: 0.7463]\n",
            "Epoch 49 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.9481 - train acc: 0.7029]\n",
            "Epoch 49 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.98batch/s, val loss: 0.9568 - val acc: 0.6817]\n",
            "Epoch 50 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.9269 - train acc: 0.7118]\n",
            "Epoch 50 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.72batch/s, val loss: 0.7645 - val acc: 0.7633]\n",
            "Epoch 51 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.9238 - train acc: 0.7091]\n",
            "Epoch 51 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.87batch/s, val loss: 0.8404 - val acc: 0.7448]\n",
            "Epoch 52 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.9265 - train acc: 0.7074]\n",
            "Epoch 52 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.93batch/s, val loss: 0.7557 - val acc: 0.7611]\n",
            "Epoch 53 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.9200 - train acc: 0.7088]\n",
            "Epoch 53 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.02batch/s, val loss: 0.7498 - val acc: 0.7543]\n",
            "Epoch 54 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.9257 - train acc: 0.7101]\n",
            "Epoch 54 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.67batch/s, val loss: 0.7598 - val acc: 0.7543]\n",
            "Epoch 55 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.9009 - train acc: 0.7206]\n",
            "Epoch 55 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.79batch/s, val loss: 0.6928 - val acc: 0.7737]\n",
            "Epoch 56 Training: 100%|██████████| 250/250 [00:58<00:00,  4.24batch/s, train loss: 0.9051 - train acc: 0.7142]\n",
            "Epoch 56 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.02batch/s, val loss: 0.7826 - val acc: 0.7413]\n",
            "Epoch 57 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.9027 - train acc: 0.7138]\n",
            "Epoch 57 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.02batch/s, val loss: 0.7674 - val acc: 0.7665]\n",
            "Epoch 58 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8933 - train acc: 0.7192]\n",
            "Epoch 58 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.85batch/s, val loss: 0.8088 - val acc: 0.7389]\n",
            "Epoch 59 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.8913 - train acc: 0.7234]\n",
            "Epoch 59 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s, val loss: 0.7408 - val acc: 0.7788]\n",
            "Epoch 60 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8857 - train acc: 0.7237]\n",
            "Epoch 60 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.10batch/s, val loss: 0.7673 - val acc: 0.7602]\n",
            "Epoch 61 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.8930 - train acc: 0.7213]\n",
            "Epoch 61 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.93batch/s, val loss: 0.7489 - val acc: 0.7619]\n",
            "Epoch 62 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8852 - train acc: 0.7253]\n",
            "Epoch 62 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.91batch/s, val loss: 0.8372 - val acc: 0.7256]\n",
            "Epoch 63 Training: 100%|██████████| 250/250 [00:58<00:00,  4.25batch/s, train loss: 0.8878 - train acc: 0.7207]\n",
            "Epoch 63 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.72batch/s, val loss: 0.7812 - val acc: 0.7560]\n",
            "Epoch 64 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8799 - train acc: 0.7245]\n",
            "Epoch 64 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.10batch/s, val loss: 0.6983 - val acc: 0.7853]\n",
            "Epoch 65 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8656 - train acc: 0.7304]\n",
            "Epoch 65 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.00batch/s, val loss: 0.8509 - val acc: 0.7198]\n",
            "Epoch 66 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8633 - train acc: 0.7298]\n",
            "Epoch 66 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.7539 - val acc: 0.7654]\n",
            "Epoch 67 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.8677 - train acc: 0.7328]\n",
            "Epoch 67 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.94batch/s, val loss: 0.7235 - val acc: 0.7663]\n",
            "Epoch 68 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8590 - train acc: 0.7309]\n",
            "Epoch 68 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s, val loss: 0.7028 - val acc: 0.7762]\n",
            "Epoch 69 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8626 - train acc: 0.7301]\n",
            "Epoch 69 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.92batch/s, val loss: 0.7277 - val acc: 0.7726]\n",
            "Epoch 70 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8529 - train acc: 0.7325]\n",
            "Epoch 70 Validation: 100%|██████████| 28/28 [00:04<00:00,  7.00batch/s, val loss: 0.7554 - val acc: 0.7484]\n",
            "Epoch 71 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8494 - train acc: 0.7334]\n",
            "Epoch 71 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.86batch/s, val loss: 0.7028 - val acc: 0.7727]\n",
            "Epoch 72 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8407 - train acc: 0.7363]\n",
            "Epoch 72 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.77batch/s, val loss: 0.8326 - val acc: 0.7303]\n",
            "Epoch 73 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8340 - train acc: 0.7410]\n",
            "Epoch 73 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.95batch/s, val loss: 0.8169 - val acc: 0.7471]\n",
            "Epoch 74 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8468 - train acc: 0.7383]\n",
            "Epoch 74 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.94batch/s, val loss: 0.7794 - val acc: 0.7623]\n",
            "Epoch 75 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.8435 - train acc: 0.7405]\n",
            "Epoch 75 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.85batch/s, val loss: 0.7445 - val acc: 0.7761]\n",
            "Epoch 76 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8273 - train acc: 0.7439]\n",
            "Epoch 76 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.93batch/s, val loss: 0.6496 - val acc: 0.7994]\n",
            "Epoch 77 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8273 - train acc: 0.7412]\n",
            "Epoch 77 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.07batch/s, val loss: 0.7971 - val acc: 0.7402]\n",
            "Epoch 78 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8412 - train acc: 0.7392]\n",
            "Epoch 78 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.6652 - val acc: 0.7878]\n",
            "Epoch 79 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.8290 - train acc: 0.7422]\n",
            "Epoch 79 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.70batch/s, val loss: 0.6920 - val acc: 0.7764]\n",
            "Epoch 80 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8178 - train acc: 0.7437]\n",
            "Epoch 80 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.98batch/s, val loss: 0.8258 - val acc: 0.7353]\n",
            "Epoch 81 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8229 - train acc: 0.7407]\n",
            "Epoch 81 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.7329 - val acc: 0.7747]\n",
            "Epoch 82 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8223 - train acc: 0.7449]\n",
            "Epoch 82 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.06batch/s, val loss: 0.8110 - val acc: 0.7530]\n",
            "Epoch 83 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8087 - train acc: 0.7507]\n",
            "Epoch 83 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.91batch/s, val loss: 0.6608 - val acc: 0.7845]\n",
            "Epoch 84 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.8168 - train acc: 0.7472]\n",
            "Epoch 84 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.89batch/s, val loss: 0.7405 - val acc: 0.7660]\n",
            "Epoch 85 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.8143 - train acc: 0.7432]\n",
            "Epoch 85 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.03batch/s, val loss: 0.6892 - val acc: 0.7850]\n",
            "Epoch 86 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.8167 - train acc: 0.7454]\n",
            "Epoch 86 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.06batch/s, val loss: 0.6647 - val acc: 0.7872]\n",
            "Epoch 87 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.8151 - train acc: 0.7487]\n",
            "Epoch 87 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.88batch/s, val loss: 0.6672 - val acc: 0.7937]\n",
            "Epoch 88 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7964 - train acc: 0.7534]\n",
            "Epoch 88 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.95batch/s, val loss: 0.6784 - val acc: 0.7875]\n",
            "Epoch 89 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7956 - train acc: 0.7525]\n",
            "Epoch 89 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.96batch/s, val loss: 0.7531 - val acc: 0.7572]\n",
            "Epoch 90 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7955 - train acc: 0.7522]\n",
            "Epoch 90 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.05batch/s, val loss: 0.6572 - val acc: 0.7918]\n",
            "Epoch 91 Training: 100%|██████████| 250/250 [00:59<00:00,  4.23batch/s, train loss: 0.8035 - train acc: 0.7503]\n",
            "Epoch 91 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.62batch/s, val loss: 0.6286 - val acc: 0.7998]\n",
            "Epoch 92 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7972 - train acc: 0.7573]\n",
            "Epoch 92 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.78batch/s, val loss: 0.7121 - val acc: 0.7693]\n",
            "Epoch 93 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7997 - train acc: 0.7482]\n",
            "Epoch 93 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.6889 - val acc: 0.7857]\n",
            "Epoch 94 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7836 - train acc: 0.7542]\n",
            "Epoch 94 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.99batch/s, val loss: 0.6363 - val acc: 0.8020]\n",
            "Epoch 95 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7761 - train acc: 0.7588]\n",
            "Epoch 95 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.65batch/s, val loss: 0.7414 - val acc: 0.7538]\n",
            "Epoch 96 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.7815 - train acc: 0.7542]\n",
            "Epoch 96 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.92batch/s, val loss: 0.7811 - val acc: 0.7534]\n",
            "Epoch 97 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7814 - train acc: 0.7550]\n",
            "Epoch 97 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.7008 - val acc: 0.7813]\n",
            "Epoch 98 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.7805 - train acc: 0.7599]\n",
            "Epoch 98 Validation: 100%|██████████| 28/28 [00:04<00:00,  7.00batch/s, val loss: 0.6921 - val acc: 0.7826]\n",
            "Epoch 99 Training: 100%|██████████| 250/250 [00:59<00:00,  4.17batch/s, train loss: 0.7800 - train acc: 0.7580]\n",
            "Epoch 99 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.66batch/s, val loss: 0.7293 - val acc: 0.7672]\n",
            "Epoch 100 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7776 - train acc: 0.7582]\n",
            "Epoch 100 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.84batch/s, val loss: 0.6563 - val acc: 0.7963]\n",
            "Epoch 101 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7720 - train acc: 0.7600]\n",
            "Epoch 101 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.96batch/s, val loss: 0.6548 - val acc: 0.8059]\n",
            "Epoch 102 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7868 - train acc: 0.7559]\n",
            "Epoch 102 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.94batch/s, val loss: 0.6532 - val acc: 0.7944]\n",
            "Epoch 103 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7616 - train acc: 0.7646]\n",
            "Epoch 103 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.58batch/s, val loss: 0.6955 - val acc: 0.7751]\n",
            "Epoch 104 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7695 - train acc: 0.7614]\n",
            "Epoch 104 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.93batch/s, val loss: 0.6875 - val acc: 0.7838]\n",
            "Epoch 105 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7685 - train acc: 0.7637]\n",
            "Epoch 105 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.7016 - val acc: 0.7817]\n",
            "Epoch 106 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7701 - train acc: 0.7619]\n",
            "Epoch 106 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.88batch/s, val loss: 0.6180 - val acc: 0.8062]\n",
            "Epoch 107 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7713 - train acc: 0.7625]\n",
            "Epoch 107 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.68batch/s, val loss: 0.6429 - val acc: 0.7993]\n",
            "Epoch 108 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7492 - train acc: 0.7677]\n",
            "Epoch 108 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.95batch/s, val loss: 0.6863 - val acc: 0.7880]\n",
            "Epoch 109 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7657 - train acc: 0.7649]\n",
            "Epoch 109 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.86batch/s, val loss: 0.6184 - val acc: 0.8039]\n",
            "Epoch 110 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7687 - train acc: 0.7632]\n",
            "Epoch 110 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s, val loss: 0.6384 - val acc: 0.8021]\n",
            "Epoch 111 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7659 - train acc: 0.7649]\n",
            "Epoch 111 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s, val loss: 0.6437 - val acc: 0.7880]\n",
            "Epoch 112 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7427 - train acc: 0.7693]\n",
            "Epoch 112 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.94batch/s, val loss: 0.6907 - val acc: 0.7807]\n",
            "Epoch 113 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7600 - train acc: 0.7670]\n",
            "Epoch 113 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.91batch/s, val loss: 0.6731 - val acc: 0.7815]\n",
            "Epoch 114 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7506 - train acc: 0.7695]\n",
            "Epoch 114 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s, val loss: 0.6242 - val acc: 0.7985]\n",
            "Epoch 115 Training: 100%|██████████| 250/250 [00:59<00:00,  4.22batch/s, train loss: 0.7558 - train acc: 0.7669]\n",
            "Epoch 115 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s, val loss: 0.6045 - val acc: 0.8131]\n",
            "Epoch 116 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7563 - train acc: 0.7706]\n",
            "Epoch 116 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.01batch/s, val loss: 0.6556 - val acc: 0.7926]\n",
            "Epoch 117 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7500 - train acc: 0.7663]\n",
            "Epoch 117 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s, val loss: 0.6079 - val acc: 0.8035]\n",
            "Epoch 118 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7577 - train acc: 0.7665]\n",
            "Epoch 118 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.88batch/s, val loss: 0.6633 - val acc: 0.7978]\n",
            "Epoch 119 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7530 - train acc: 0.7692]\n",
            "Epoch 119 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.01batch/s, val loss: 0.6347 - val acc: 0.8045]\n",
            "Epoch 120 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7345 - train acc: 0.7735]\n",
            "Epoch 120 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.84batch/s, val loss: 0.6544 - val acc: 0.7899]\n",
            "Epoch 121 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7466 - train acc: 0.7690]\n",
            "Epoch 121 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.77batch/s, val loss: 0.6075 - val acc: 0.8057]\n",
            "Epoch 122 Training: 100%|██████████| 250/250 [00:59<00:00,  4.21batch/s, train loss: 0.7416 - train acc: 0.7683]\n",
            "Epoch 122 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.93batch/s, val loss: 0.6173 - val acc: 0.8121]\n",
            "Epoch 123 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7361 - train acc: 0.7737]\n",
            "Epoch 123 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s, val loss: 0.6536 - val acc: 0.7885]\n",
            "Epoch 124 Training: 100%|██████████| 250/250 [00:59<00:00,  4.17batch/s, train loss: 0.7441 - train acc: 0.7709]\n",
            "Epoch 124 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.84batch/s, val loss: 0.6122 - val acc: 0.8031]\n",
            "Epoch 125 Training: 100%|██████████| 250/250 [00:59<00:00,  4.20batch/s, train loss: 0.7460 - train acc: 0.7671]\n",
            "Epoch 125 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.99batch/s, val loss: 0.6214 - val acc: 0.8043]\n",
            "Epoch 126 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7361 - train acc: 0.7761]\n",
            "Epoch 126 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.72batch/s, val loss: 0.6739 - val acc: 0.7829]\n",
            "Epoch 127 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7328 - train acc: 0.7755]\n",
            "Epoch 127 Validation: 100%|██████████| 28/28 [00:03<00:00,  7.03batch/s, val loss: 0.6145 - val acc: 0.8026]\n",
            "Epoch 128 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7397 - train acc: 0.7717]\n",
            "Epoch 128 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.86batch/s, val loss: 0.6442 - val acc: 0.7975]\n",
            "Epoch 129 Training: 100%|██████████| 250/250 [00:59<00:00,  4.19batch/s, train loss: 0.7221 - train acc: 0.7773]\n",
            "Epoch 129 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.76batch/s, val loss: 0.6115 - val acc: 0.8130]\n",
            "Epoch 130 Training: 100%|██████████| 250/250 [00:59<00:00,  4.18batch/s, train loss: 0.7369 - train acc: 0.7725]\n",
            "Epoch 130 Validation: 100%|██████████| 28/28 [00:04<00:00,  6.97batch/s, val loss: 0.6379 - val acc: 0.8040]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "top1 = []\n",
        "train_time = []\n",
        "test_time = []\n",
        "train_acc = []\n",
        "train_loss = []\n",
        "val_acc = []\n",
        "val_loss = []\n",
        "counter = 0\n",
        "epoch = 0\n",
        "\n",
        "while counter < 15:   # Counter number of epochs of non-improvement before stopping\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    running_train_acc = 0.0\n",
        "    train_batch_count = 0\n",
        "    start_train_time = time.time()\n",
        "\n",
        "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1} Training\") as tepoch:\n",
        "        for inputs, labels in tepoch:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                acc = (outputs.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Loss and accuracy tracking\n",
        "            running_train_loss += loss.item()\n",
        "            running_train_acc += acc.item()\n",
        "            train_batch_count += 1\n",
        "\n",
        "            tepoch.set_postfix_str(f\"train loss: {running_train_loss / train_batch_count:.4f} - train acc: {running_train_acc / train_batch_count:.4f}\")\n",
        "\n",
        "\n",
        "    end_train_time = time.time()\n",
        "    train_time.append(end_train_time - start_train_time)\n",
        "    train_acc.append(running_train_acc / train_batch_count)\n",
        "    train_loss.append(running_train_loss / train_batch_count)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    running_val_acc = 0.0\n",
        "    val_batch_count = 0\n",
        "    val_y_true = []\n",
        "    val_y_pred = []\n",
        "    start_test_time = time.time()\n",
        "\n",
        "    with torch.no_grad(), tqdm(test_loader, unit=\"batch\", desc=f\"Epoch {epoch+1} Validation\") as vepoch:\n",
        "        for images, labels in vepoch:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = (outputs.argmax(dim=1) == labels.argmax(dim=1)).float().mean()\n",
        "\n",
        "            # Update lists for classification report\n",
        "            val_y_true.extend(labels.cpu().tolist())\n",
        "            val_y_pred.extend(outputs.argmax(dim=1).cpu().tolist())\n",
        "\n",
        "            # Loss and accuracy tracking\n",
        "            running_val_loss += loss.item()\n",
        "            running_val_acc += acc.item()\n",
        "            val_batch_count += 1\n",
        "\n",
        "            # Update progress bar postfix\n",
        "            vepoch.set_postfix_str(f\"val loss: {running_val_loss / val_batch_count:.4f} - val acc: {running_val_acc / val_batch_count:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    end_test_time = time.time()\n",
        "    test_time.append(end_test_time - start_test_time)\n",
        "    val_acc.append(running_val_acc / val_batch_count)\n",
        "    val_loss.append(running_val_loss / val_batch_count)\n",
        "\n",
        "    current_val_acc = running_val_acc / val_batch_count\n",
        "    top1.append(current_val_acc)\n",
        "\n",
        "    # Early stopping condition\n",
        "    if current_val_acc >= max(top1[:-1], default=0):\n",
        "        torch.save(model.state_dict(), save_path)  # Replace with your model save path\n",
        "        # print(\"Model improved and saved.\")\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    if counter >= 15:\n",
        "        print(\"\\nEarly stopping triggered.\")\n",
        "        break\n",
        "\n",
        "    epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD56UUbNJOVC",
        "outputId": "faf713f3-614f-40a4-9441-eaca5eaa3702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n",
            "Results\n",
            "Top 1 Accuracy: 0.81 - Train Time: 59 -Test Time: 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Finished Training')\n",
        "print(\"Results\")\n",
        "print(f\"Top 1 Accuracy: {max(top1):.2f} - Train Time: {min(train_time):.0f} -Test Time: {min(test_time):.0f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQDwGbSGlyWR"
      },
      "outputs": [],
      "source": [
        "val_y_true = [np.argmax(y, axis=None) for y in val_y_true]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S799m-u2JskM",
        "outputId": "4c9ea5bf-744e-44e5-c4e2-9e11d78efab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.8044\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "overall_accuracy = accuracy_score(val_y_true, val_y_pred)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBdH0mSeD7gH",
        "outputId": "ffcdc6bc-f0d5-43c9-ae8b-4e2eb699b89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.19      0.29       111\n",
            "           1       0.83      0.86      0.85       179\n",
            "           2       0.97      0.87      0.92       264\n",
            "           3       0.83      0.94      0.88       213\n",
            "           4       0.68      0.78      0.72        32\n",
            "           5       0.96      0.75      0.84       203\n",
            "           6       0.70      0.83      0.76       193\n",
            "           7       0.62      0.76      0.68       251\n",
            "           8       0.79      0.94      0.86       147\n",
            "           9       0.91      0.86      0.88       181\n",
            "\n",
            "    accuracy                           0.80      1774\n",
            "   macro avg       0.79      0.78      0.77      1774\n",
            "weighted avg       0.81      0.80      0.80      1774\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print classification report\n",
        "print(classification_report(val_y_true, val_y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "a1s_KHXtJNXD",
        "outputId": "b335117f-13a9-4f33-8c0a-53c6ed03c1e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAKnCAYAAAAfqgv+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACNUklEQVR4nOzdd3wU1f7G8WdTSEhoUkKTEkCqAgKCoegVkCpFEERBEQEbIEVAuah0g4CCoIIK0gRpigooLSBSJXTp0iGUEEICoaTt/v7gZ8xe6i7ZPUn2876veV12djLzjCfZ3bPfc2YsNpvNJgAAAABwgpfpAAAAAAAyLjoUAAAAAJxGhwIAAACA0+hQAAAAAHAaHQoAAAAATqNDAQAAAMBpdCgAAAAAOI0OBQAAAACn0aEAAAAA4DQf0wFcoWTeKqYjGHHycqTpCEZYLBbTEYzI7Z/ddAQjLiVcNR3BiISkRNMRjPDzyWI6ghH18lYwHcGIsKg9piMYkZDsmX/fCfGnTEe4rcSoI8aO7Zu3hLFjO4sKBQAAAACnZcoKBQAAAOA0a7LpBBkKFQoAAAAATqNDAQAAAMBpDHkCAAAAUrNZTSfIUKhQAAAAAHAaFQoAAAAgNSsVCkdQoQAAAADgNCoUAAAAQCo25lA4hAoFAAAAAKfRoQAAAADgNIY8AQAAAKkxKdshVCgAAAAAOI0KBQAAAJAak7IdQoUCAAAAgNPoUAAAAABwGkOeAAAAgNSsyaYTZChUKAAAAAA4jQoFAAAAkBqTsh1ChQIAAACA06hQAAAAAKlxYzuHUKEAAAAA4DQ6FAAAAACcxpAnAAAAIBUbk7IdQoXCQW/07KSFK2Zq57G12rxvpSbN+ETBpYrZbdPu5Vaa9fPX2nH0Dx2O2qbsObIZSuta/ft104b1i3Uhar9OndyhBfMnq3TpEqZjudxrXV/SlvDlOh+5V+cj92rN7z+pYYP/mI6V5h6vWVXT53yh7ft+15mYvWrUtJ7d8+O+HKEzMXvtltkLvjKU1rUKFsqvKVPG6sTJ7Yq6sF+bNy/Vo1UeMR3LpWrXrqGFC6fp+LGtSkyIUPPmDU1Hcos9+9Yq7urRm5ZPxw41Hc1lWr/1nH4+sVidB3WVJAU9GKSfTyy+5VKzaS3DadOWJ7a35Lnv33AdKhQOqlGzqr6bMk+7tu+Rt4+3+r7fXdPnf6mGtVrr2tXrkiT/rP76I2yD/gjboP4fvm04sevUeSJEEydN19YtO+Xj462hw97TksWzVanyU7p69ZrpeC4TEXFG778fqkOHjspisajDS220YMEUVa/RWPv2HTQdL80EBARo718HNOe7H/XtdxNuuc2qFWvVq9vAlMcJ8Qnuiuc2uXLlUFjYD/rjj4169tlXFHX+gkqVClbMxVjT0VwqMDBAu3bt1bRpc7Rg/hTTcdzmyTot5OX973dt5cuX0eIl32nhj0sMpnKdUhUfUsMXG+no3qMp66JOR6lj1Q522zV8sZGefb2Vtq3e6u6ILuVp7f0PT33/dgiTsh1Ch8JBnZ7vbve4f/dBCj+wSg9XKq/wjdskSdO+mi1JqlGrqtvzuVOzZvZvOF269NbpiF2qUqWi1q3701Aq11vy60q7x4MGjdJrXV9SjRqPZqoOxaqVa7Vq5do7bpOQkKDzkVFuSmRGnz5v6tSp03rj9X4p644fP2UwkXssW7Zay5atNh3D7aKiou0ev/POmzp8+JjWrs18r2n+Af7qM76vvnhvgtr0aJey3mq1KuZ8jN22jzcM0brF63T9/784yyw8qb1T89T3b7iO0SFPUVFRGjVqlJ599lmFhIQoJCREzz77rEaPHq3z58+bjHbPsufILkmKzeTfVt6LnDlzSJIuRseYDeJGXl5eatOmuQIDs2rTpm2m47hdSO3H9Nffa7U2fIlGfvKhHnggp+lIaa5J0/ravu0vzfzuCx07tkUbNi7RK53a3f0HkeH5+vqqXbuWmjljvukoLvH68De1dVW4dq7becftSj5SUiUeLqmVc5e7KZkZmb2978QT37+RtoxVKMLDw9WwYUMFBASofv36Kl26tCTp3LlzGj9+vEaOHKlly5apWrVqpiLelcVi0fsj+mrLpu06uP+w6ThGWSwWjRkzWOvXb9aevQdMx3G5ChXK6o81P8nf309xcVfUtm1X7d//t+lYbrV65Tr9umilThw/peLBRTXgg16ateArPfP0i7JmolJxcHBRdenaQRMmTNaY0V+qStWKGjNmsBITEjVr1g+m48GFmjVroJy5cui77xaYjpLm6jR7QiUeLqm+zXrfddv6zzfQyb9PaP/W/W5IZk5mbu878bT373vGpGyHGOtQ9OjRQ23atNGkSZNksVjsnrPZbHrjjTfUo0cPbdy48Y77iY+PV3x8/P/8vFUWi+uLL0NGvafSZUvq+aavuvxY6d348SNUoXwZPVW3lekobnHw4GFVr95IOXJmV6tWTTR58ljVf7qNR3Uqfv7xt5R/79/7t/buPqA/dy5XzdrVte6PTQaTpS0vL4u2bftLgweNliTt3LlH5cuXVucu7elQZHIvd2yr5cvX6OyZSNNR0lTegnnVZXBXfdj+AyXGJ95x2yx+WfREiyc1b/xcN6UzJ7O299142vs3XMNYh2Lnzp2aNm3aTZ0J6UZvuXfv3nr00Ufvup/Q0FANGTLEbl2urAWUO6BgmmW9lUEj31XdBnXUrlkXj3vx+V/jxg1Xk8b1Va9+a0VEnDEdxy0SExN1+MgxSdL27X+pWtVK6tH9VXXrPsBsMINOHD+lC1HRCi5RNFN1KM6ejbypo3jgwGG1bNnYUCK4Q5EihfVU3Vp68YU3TUdJcyUfKaVc+R7Q2F8/S1nn7eOtCjUqqGnHZ/RcqWdTqow1m9aSX1Y/rf4hzFRct8jM7X0nnvj+fc+syaYTZCjGOhQFChTQ5s2bVbZs2Vs+v3nzZuXPn/+u+xkwYID69Oljt65y8BNpkvF2Bo18Vw2aPqX2Lbrq1InTLj1Wejdu3HC1aN5ITzdoo2PHTpqOY4zFy0tZ/PxMxzCqYKH8eiB3Lp07lzHmP92rTRu36qGH7C+n+FCpYJ04EWEoEdzhpZef0/nzF7T0t1Wmo6S5Xet3qkf9bnbr3v6kp04dPqUfv/zBbshi/ecbKHzlZl2KvuTumG6Vmdv7dnj/Rloy1qHo27evXnvtNW3dulX16tVL6TycO3dOYWFh+uabbzRmzJi77sfPz09+//NBzpXDnYaMek/NWzfW6y/1VlzcVeUNyiNJunwpTvHXbwy9yhuUR/mC8qhYcBFJUpnyD+lK3BWdPnVWsTGZ50V5/PgRavd8S7V+rrMuX45T/vz5JEmxsZd1/XrmuhJIasOGvatly37XyZMRypYtm9q1a6EnnwjRM/9z1YyMLiAwQMEliqY8LlqssCo8UlYxF2N18WKs3nn3LS35ZbkiI6NUvHhRfTD0HR09ckK/h60zmDrtTfh8ilat+kF9+72lH39YomrVKqnTqy+oRyavRgUGBqhUqeCUx8HFi6pSpQqKjr6okycz9xcp/1wOetZ3Pyg5OfN9S3ntyjWdOHjcbt31q/G6fPGy3foCxQqqQo0KGtpxsJsTuldmb+9b8dT3b4cwh8IhFpvNZjN18Llz52rs2LHaunVryh+xt7e3qlatqj59+qht27ZO7bdk3ippGdPO4ahbX8mnf/dB+mHOIknS2/1fV8/+r99xG1c4edm9Q68S4m996czOXXpr5kz3XSXjVsPmXGnSpNF66qlaKlggSLGxl7V79z6N+WSiwsLufInVtJbbP7tL9x9S+zH9uHj6Tevnzl6o9/oM1dRZE/RwxXLKkTOHzp2N1JpV6/XxiAmKOn/BpbkuJVx16f5vpVHjuho6pL9KlgrWsWMnNWHCZE2bOsetGRKS7jzWPa098USIwlbePDl1xox56tzl7hN504qfTxa3HesfdevV0S+LZqhyxbo6dOjo3X/ABerlreDW4w2fG6qje49oypBvUtZ16P+y/vPsf9S1Zme566NCWNQetxwntfTQ3gnJ7v37Ti/v37fLkR7E7zN32Wy/ck8ZO7azjHYo/pGYmKioqBvXss+bN698fX3va3+u7FCkZ+7uUKQX7u5QpBeu7lCkVyY6FOmBuzsU6YWJDkV64O4ORXphokORHri7Q5Fe0KG4tYzYoUgXN7bz9fVVwYKunUQNAAAA3JNMdPlzdzB6YzsAAAAAGVu6qFAAAAAA6QaTsh1ChQIAAACA0+hQAAAAAHAaQ54AAACA1JiU7RAqFAAAAACcRoUCAAAASMVm84y7pqcVKhQAAAAAnEaFAgAAAEiNy8Y6hAoFAAAAAKfRoQAAAADgNIY8AQAAAKlx2ViHUKEAAAAA4DQqFAAAAEBqTMp2CBUKAAAAAE6jQwEAAADAaQx5AgAAAFKzcqdsR1ChAAAAAOA0KhQAAABAakzKdggVCgAAAABOo0IBAAAApMaN7RxChQIAAACA0+hQAAAAAHAaQ54AAACA1JiU7ZBM2aE4eTnSdAQjol4ubzqCEbmn7zEdwYioq7GmIxhhMx3AEIvpAIZcT0owHcGIPy8dNh3BiITkRNMRjPD28jYdAbgvmbJDAQAAADiNSdkOYQ4FAAAAAKfRoQAAAADgNIY8AQAAAKkx5MkhVCgAAAAAOI0KBQAAAJCKzZZsOkKGQoUCAAAAgNPoUAAAAABwGkOeAAAAgNSYlO0QKhQAAAAAnEaFAgAAAEjNRoXCEVQoAAAAADiNDgUAAACQmtVqbnFAaGioHnvsMWXPnl1BQUFq2bKlDhw4YLfN9evX1a1bN+XJk0fZsmVT69atde7cObttTpw4oaZNmyogIEBBQUHq16+fkpKS7jkHHQoAAAAgA1qzZo26deumTZs2acWKFUpMTFSDBg105cqVlG169+6tRYsWaf78+VqzZo1Onz6tVq1apTyfnJyspk2bKiEhQRs2bND06dM1bdo0ffjhh/ecw2Kz2WxpembpQBa/B01HMCLq5fKmIxiRe/oe0xGMsJgOYEime8G6R7S3Z8kbkMN0BCOir102HcEIby9v0xGMuHbtuOkIt3Ut7Gtjx85a7zWnf/b8+fMKCgrSmjVr9MQTTyg2Nlb58uXT7Nmz9dxzz0mS9u/fr3Llymnjxo16/PHH9dtvv+mZZ57R6dOnlT9/fknSpEmT9O677+r8+fPKkiXLXY9LhQIAAABIzWY1tsTHx+vSpUt2S3x8/D3Fjo2NlSTlzp1bkrR161YlJiaqfv36KduULVtWRYsW1caNGyVJGzdu1COPPJLSmZCkhg0b6tKlS9qz596+tKVDAQAAAKQToaGhypkzp90SGhp615+zWq3q1auXatWqpYcffliSdPbsWWXJkkW5cuWy2zZ//vw6e/ZsyjapOxP/PP/Pc/eCy8YCAAAAqRm8sd2AAQPUp08fu3V+fn53/blu3bpp9+7dWrdunaui3RYdCgAAACCd8PPzu6cORGrdu3fX4sWL9ccff+jBB/+dS1ygQAElJCQoJibGrkpx7tw5FShQIGWbzZs32+3vn6tA/bPN3TDkCQAAAMiAbDabunfvroULF2rVqlUKDg62e75q1ary9fVVWFhYyroDBw7oxIkTCgkJkSSFhITor7/+UmRkZMo2K1asUI4cOVS+/L1d8IcKBQAAAJBaBrlTdrdu3TR79mz9/PPPyp49e8qch5w5cypr1qzKmTOnOnfurD59+ih37tzKkSOHevTooZCQED3++OOSpAYNGqh8+fJ66aWXNGrUKJ09e1bvv/++unXrds+VEjoUAAAAQAY0ceJESdJ//vMfu/VTp07VK6+8IkkaO3asvLy81Lp1a8XHx6thw4b68ssvU7b19vbW4sWL9eabbyokJESBgYHq2LGjhg4des85uA9FJsJ9KDwL9yXwLLS3Z+E+FJ6F+1CkP9d+G2/s2Fkbv23s2M5iDgUAAAAAp9GhSAP9+3XThvWLdSFqv06d3KEF8yerdOkSpmPdN++HHlHW7kOVbfT3yvHNcvlUrmn3vH+nvsrxzXK7JaDniFvvzMdXgR9OVI5vlsurSMb/byNJb77RUYcOblLcpcPasG6RHqtW2XQkl6pdu4YWLpym48e2KjEhQs2bNzQdya1ob9o7M3m8ZjXNmPOlduxbo7Mx+9Soab3bbvvxp4N0Nmafur75shsTukdmff++m4EDe+nateN2y44dYXf/QU9itZpbMiA6FGmgzhMhmjhpuurUaa4mTV6Qj6+vliyerYCArKaj3ReLn7+sp47o+uzPb7tN0l/huvzO8ynL1W9ufeMVv9ZdZI254KqobtemTXONGT1Iw4Z/qsdqNNLOXXv165JZypcvj+loLhMYGKBdu/bq7Z4DTUdxO9rbs3hCewcEZNWevw5oQL9hd9yu8TP1VfWxSjpz+pybkrlXZn3/vhd79hxQ8eLVUpZ69Z4zHQkZGJOy00CzZh3sHnfp0lunI3apSpWKWrfuT0Op7l/S7nAl7Q6/4za2pETZLl284zY+Dz8mnwpVdW3iUPk+Uj0tIxrTu2dXTZ4yW9NnzJMkvdXtPTVpXE+dXmmnUaO/MJzONZYtW61ly1abjmEE7e1ZPKG9V61cq1Ur195xmwIFgzTi44F6oXVXfTdvkpuSuVdmff++F0lJSTp37rzpGMgkqFC4QM6cNybTXYyOMRvEDXzKVFS2T+YpcNgU+bfvIUtgdrvnLdlzyf/lXro25WPZEuINpUxbvr6+qlKlosJW/ftmbLPZFLZqnR5/vKrBZHAF2tuz0N43WCwWff7Vx/pywrc6sP+Q6Thu40nv36VKBevIkc3au3etpk79TEWKFDIdKX2xWc0tGRAdijRmsVg0ZsxgrV+/WXv2HjAdx6WSdm/RtW9H6eqn/RX/wxR5l654Yw6F5d9fq6yd+ilhzRJZj/9tMGnayps3t3x8fBR5LspufWTkeRXIn89QKrgK7e1ZaO8buvfqoqSkZE2eNNN0FLfxpPfv8PAdeu21d9S8+ct6++2BKl68iFaunK9s2QJNR0MGla6HPJ08eVKDBg3St99+e9tt4uPjFR9v/823zWaTxWLmIovjx49QhfJl9FTdVkaO705J4b+n/NsacUzJp44oe+gMeZepqOT9O5SlbkvJP6sSfp1jLCMAwDEVK5VX1zde0tNPtjYdxa086f17+fLfU/69e/d+hYfv0IED69W69TOaPn2uuWDpSQadHG1Kuq5QREdHa/r06XfcJjQ0VDlz5rRbrMlmrmM9btxwNWlcXw0atlVExBkjGUyyRZ2V9XKMvIIKS5K8y1aWd8lyyj5xibJP+k3ZRkyTJAUO/EL+nfoZTHp/oqKilZSUpKD8ee3WBwXl01nGo2Y6tLdnob2lGjWrKW++PNq6e5VORf2lU1F/qUjRwho8vL/Cd600Hc8lPP39Ozb2kg4dOqqSJYuZjoIMymiF4pdffrnj80eOHLnrPgYMGKA+ffrYrcuTt9x95XLGuHHD1aJ5Iz3doI2OHTvp9uOnB5YH8soSmEO22BtXc7o+5wtZfpr27/O58iiwd6iufT1CyUf3G0p5/xITE7Vt2y7Vfaq2fvllmaQbpfK6T9XWlxOnGk6HtEZ7exbaW1ow5xet/X2j3brvf/hGC+b+ojmzfjSUynV4/75xVbfg4GI6ezbztS/cw2iHomXLlrJYLLrTzbrvNnTJz89Pfn5+Dv1MWhs/foTaPd9SrZ/rrMuX45T//8fZxsZe1vXr192aJU35+csr6N9JWl55C8irSAnZrlyW7cpl+TV7SUnb1soae1Fe+QrK/7musp4/raQ9WyVJtujzsunfb/Qs8dckSdbzp2W7aD8+OaMZ+9k3mjplrLZu26Xw8O16u0dXBQZm1bRMXCoODAxQqVLBKY+DixdVpUoVFB19USdPnjaYzPVob9o7s7V3QGCAgksUTXlctNiDqvBIWcVcjFXEqTO6eDHGbvukpCRFRkbp8KFj7g3qYpn2/fsuQkMHasmSlTpxIkKFCuXX++/3VnJysubNu/MXvR4lg06ONsVoh6JgwYL68ssv1aJFi1s+v2PHDlWtmv6vqvHG6x0lSWErF9it79ylt2bOnG8iUprwLlZagf3GpDz2f/4NSVLChuW6/t14eT8YLN+Qp2UJCJQt5oKS9m5T/E/TpKREQ4ndZ/78X5Qvb24N/rCvChTIp50796jpMx0UGZmxO0p3UrVqJbvf8TFjBkuSZsyYp85dehtK5R60N+2d2dq78qMV9OPiGSmPh370niRp7uyF6vnWf03FcrvM+v59N4ULF9CMGROUO3cuRUVFa8OGcD35ZEtFRUWbjoYMymK7U3nAxZo3b67KlStr6NCht3x+586devTRR2V1cGJMFr8H0yJehhP1cnnTEYzIPX2P6QhGmLnsgHnGXrAMo709S96AHKYjGBF9zcwcSNO8vbxNRzDi2rXjpiPc1rWFI40dO+uz7xk7trOMVij69eunK1eu3Pb5UqVKafVqz7yxEgAAAJARGO1Q1KlT547PBwYG6sknn3RTGgAAAEDMoXBQur5sLAAAAID0jQ4FAAAAAKel6ztlAwAAAG7HnbIdQoUCAAAAgNOoUAAAAACpUaFwCBUKAAAAAE6jQwEAAADAaQx5AgAAAFKz2UwnyFCoUAAAAABwGhUKAAAAIDUmZTuECgUAAAAAp1GhAAAAAFKjQuEQKhQAAAAAnEaHAgAAAIDTGPIEAAAApGZjyJMjqFAAAAAAcBoVCgAAACA1JmU7hAoFAAAAAKfRoQAAAADgNIY8AQAAAKnZbKYTZChUKAAAAAA4jQoFAAAAkBqTsh1ChQIAAACA06hQAAAAAKlRoXAIHYpMJP93B0xHMOLa6bWmIxjxQNF6piMYcT0pwXQEIywWi+kIRuTJmt10BCNi46+ajmCE1UMnwlqTk0xHAO4LQ54AAAAAOI0KBQAAAJCajSFPjqBCAQAAAMBpVCgAAACAVGxWz5zP4ywqFAAAAACcRocCAAAAgNMY8gQAAACkxn0oHEKFAgAAAIDTqFAAAAAAqXHZWIdQoQAAAADgNCoUAAAAQGpcNtYhVCgAAAAAOI0OBQAAAACnMeQJAAAASI3LxjqECgUAAAAAp1GhAAAAAFKjQuEQKhQAAAAAnEaHAgAAAIDTGPIEAAAApGbjPhSOoEIBAAAAwGlUKAAAAIDUmJTtECoUAAAAAJxGhwIAAACA0xjyBAAAAKRmZVK2I6hQpIH+/bppw/rFuhC1X6dO7tCC+ZNVunQJ07FcbuDAXrp27bjdsmNHmOlY9+WbGXP1fOe3Vb1+Kz3RtJ3efm+ojh4/lfJ87KXL+ujTL/VMuy6q+lQL1W/1sj4aO1GX467Y7WfTlu1q/3ofVa/fSk82e1GffjlFSUnJ7j6dNLVn31rFXT160/Lp2KGmo7nFm2901KGDmxR36bA2rFukx6pVNh3JpTzlde3xmtU0Y86X2rFvjc7G7FOjpvVuu+3Hnw7S2Zh96vrmy25M6B6Z8fXcEZ729/0PTz1vpD06FGmgzhMhmjhpuurUaa4mTV6Qj6+vliyerYCArKajudyePQdUvHi1lKVevedMR7ovW3b8pRdaNdPsr8fq63EfKTEpSa/1Hqir165LkiKjLigyKlp9u3fRwpkTNWJgH63/c6s+DB2bso/9fx/Rm30/VO0aVbVg2ucaM/Q9rV73p8ZO+tbUaaWJJ+u0UIngx1KWZ5p2kCQt/HGJ4WSu16ZNc40ZPUjDhn+qx2o00s5de/XrklnKly+P6Wgu4ymvawEBWbXnrwMa0G/YHbdr/Ex9VX2sks6cPuemZO6X2V7P75Un/n1Lnnve98xmNbdkQBabLfNdaDeL34NGj583b26djtiluvVaa926P912XG8vb7cdS7rxjVazZg30+ONN3Hrc/3Xp5GqX7Tv6YoyeeOYFTftilKpVfuSW2yxbtVbvDR2l8JU/ycfHW+MmTdPG8G2aO2V8yja/r9ukdz4I1R+Lv1dgYECaZHug6O2/SXWHj0d9oEaN66rSI0+59bjXkxLcejxJ2rBukcK37FTPXu9LkiwWi44dCdcXX07VqNFfuCWDl8XiluPcjqnXtdxZs7vtWGdj9umV9t21dIn9N/MFCgbp15Vz9ULrrvpu3iR9PXGGvpk4w6VZYuOvunT//yu9vJ4nJie5/Zjp4e/bhPRw3kkJEW45jjOujn7V2LED+mW8LyCpULhAzpw5JEkXo2PMBnGDUqWCdeTIZu3du1ZTp36mIkUKmY6UpuKu3HhTz5nj9h9qLsddUbbAAPn43OjQJSYmyi9LFrtt/Pz8FJ+QoD0HDrkurBv5+vqqXbuWmjljvukoLufr66sqVSoqbNXalHU2m01hq9bp8cerGkzmXp70upaaxWLR5199rC8nfKsD+zPH3+/tZPbX81vx1L9vTz1vh1ht5pYMyHiH4tq1a1q3bp327t1703PXr1/XjBmu/RYorVksFo0ZM1jr12/Wnr0HTMdxqfDwHXrttXfUvPnLevvtgSpevIhWrpyvbNkCTUdLE1arVSM/+0qPViyvh0oUv+U2F2Ni9dW07/Vc88Yp62pWr6Idu/fp1xW/Kzk5WefOR2nS1NmSpKgL0e6I7nLNmjVQzlw59N13C0xHcbm8eXPLx8dHkeei7NZHRp5Xgfz5DKVyL096Xftf3Xt1UVJSsiZPmmk6iktl9tfz2/HUv29PPW+4jtGrPB08eFANGjTQiRMnZLFYVLt2bc2ZM0cFCxaUJMXGxqpTp056+eXbT4CLj49XfHy83TqbzSaLoeEB48ePUIXyZfRU3VZGju9Oy5f/nvLv3bv3Kzx8hw4cWK/WrZ/R9OlzzQVLI8M/+UKHjhzTjIljbvl83JUreqvfIJUMLqq3OndIWV+rRlW9062zho6eoAHDRiuLr69ef+VFbd2529jvZVp7uWNbLV++RmfPRJqOAjfwpNe11CpWKq+ub7ykp59sbTqKy2X213MArmW0QvHuu+/q4YcfVmRkpA4cOKDs2bOrVq1aOnHixD3vIzQ0VDlz5rRbrMmXXZj69saNG64mjeurQcO2iog4YySDSbGxl3To0FGVLFnMdJT7NuKTL7Vmw2Z9O+FjFQi6+duaK1eu6vU+HygwIKs+++gD+frY9807tmuljcsWaMUPM7T217l6qs7jkqQHCxdwS35XKlKksJ6qW0vTp3nGh4yoqGglJSUpKH9eu/VBQfl09tx5Q6ncx5Nf12rUrKa8+fJo6+5VOhX1l05F/aUiRQtr8PD+Ct+10nQ8l8pMr+d34ql/35563o6wWa3GlozIaIdiw4YNCg0NVd68eVWqVCktWrRIDRs2VJ06dXTkyJF72seAAQMUGxtrt3h5u28S3z/GjRuuFs0bqWGj53Xs2Em3Hz89CAwMUHBwMZ09m3G/tbbZbBrxyZcK+2ODvh0/Ug8WurkDEHflil7rPVC+vj6a8PEg+fllucWebgwTCcqXR/5+fvptxe8qkD+fypcu5epTcLmXXn5O589f0NLfVpmO4haJiYnatm2X6j5VO2WdxWJR3adqa9OmrQaTuZ6nv64tmPOL6tZqqfp1WqUsZ06f05fjv1W7Vl1Mx3OpzPB6fi889e/bU88brmN0yNO1a9fkk+qbXYvFookTJ6p79+568sknNXv27Lvuw8/PT35+fnbr3D2sZPz4EWr3fEu1fq6zLl+OU/7/H38YG3tZ169fd2sWdwoNHaglS1bqxIkIFSqUX++/31vJycmaN+8X09GcNvyTL/Trit81fuSHCgzImjLnIVu2QPn7+d3oTPQaqGvx8frsw366cuWqrvz/xO0HcuWUt/eNidnfzlqg2o9XlZfFSyvXrNfk7+brk2EDUp7PqCwWizq81EazvvtByckZ+74ajhj72TeaOmWstm7bpfDw7Xq7R1cFBmbVtEw8FMRTXtcCAgMUXKJoyuOixR5UhUfKKuZirCJOndHFizF22yclJSkyMkqHDx1zb1AXy4yv5/fKE/++Jc8973uWQSdHm2K0Q1G2bFlt2bJF5cqVs1v/+eefS5KaN29uIpbD3ni9oyQpbKX9BNXOXXpr5szMexWcwoULaMaMCcqdO5eioqK1YUO4nnyypaKiMu7E47kLb9xToVP3d+3WD/9vH7Vs+rT2HjisXf8/KbXJ853ttlm2YJoKF8wvSVq3aYu+mTFHCQmJKlMqWBNGfqg6IY+54Qxc66m6tVW0aGGPuLpTavPn/6J8eXNr8Id9VaBAPu3cuUdNn+mgyMiou/9wBuUpr2uVH62gHxf/e/GPoR+9J0maO3uher71X1Ox3C4zvp7fK0/8+5Y897zhGkbvQxEaGqq1a9fq119/veXzb731liZNmiSrg+PJTN+HwhR334civXDlfSjSM9P3oTDFxH0o0gPT96EwxZ33oUhP3H0fivTCxH0oYE56vg/FlRG3vyCQqwUOzFhXOJW4sV2mQofCs9Ch8Cx0KDwLHQp4gnTdoRje4e4buUjg+98ZO7azjN+HAgAAAEDGZXQOBQAAAJDuMCnbIVQoAAAAADiNCgUAAACQWga9wZwpVCgAAAAAOI0OBQAAAACnMeQJAAAASI1J2Q6hQgEAAADAaVQoAAAAgNRsTMp2BBUKAAAAAE6jQwEAAADAaQx5AgAAAFJjUrZDqFAAAAAAcBoVCgAAACAVG3fKdggVCgAAAABOo0IBAAAApMYcCodQoQAAAADgNDoUAAAAAJzGkCcAAAAgNYY8OYQKBQAAAACnUaEAAAAAUrNx2VhHUKEAAAAA4DQ6FAAAAACcxpAnAAAAIDUmZTuECgUAAAAAp2XKCoXN5pm9Sl8vb9MRjMhaqI7pCEZc/vUD0xGMyN5kmOkIRlg99HXt4vU40xGMSLYyIdSTWEwHwE1sVCgcQoUCAAAAgNMyZYUCAAAAcBoVCodQoQAAAADgNDoUAAAAAJzGkCcAAAAgNS6M4BAqFAAAAACcRoUCAAAASI1J2Q6hQgEAAADAaXQoAAAAADiNIU8AAABAagx5cggVCgAAAABOo0IBAAAApGKzUaFwBBUKAAAAAE6jQgEAAACkxhwKh1ChAAAAAOA0OhQAAAAAnMaQJwAAACA1hjw5hAoFAAAAAKdRoQAAAABSsVGhcAgVCgAAAABOo0MBAAAAwGkMeQIAAABSY8iTQ6hQAAAAABnQH3/8oWbNmqlQoUKyWCz66aef7J5/5ZVXZLFY7JZGjRrZbRMdHa327dsrR44cypUrlzp37qy4uDiHctChAAAAAFKzGlwccOXKFVWqVElffPHFbbdp1KiRzpw5k7J8//33ds+3b99ee/bs0YoVK7R48WL98ccfeu211xzKQYciDdSuXUMLF07T8WNblZgQoebNG5qO5BZeXl56/4Pe2rVnjc5F7dXOv1ar/7vdTcdymzff6KhDBzcp7tJhbVi3SI9Vq2w6ktOmLNusFz+erZp9PtdT705Sr69+0bFz0XbbxCcm6aO5q/Rk/4kK6f253vlmkS5cumK3zZnoS+r+5U96vNcEPfXuJH364x9KSnbw1TGdykzt7QhPO+/Xur6kLeHLdT5yr85H7tWa339Swwb/MR3LbTytvf/haeftqZ9bMqPGjRtr+PDhevbZZ2+7jZ+fnwoUKJCyPPDAAynP7du3T0uXLtXkyZNVo0YN1a5dWxMmTNCcOXN0+vTpe85BhyINBAYGaNeuvXq750DTUdyqd5831LlLe/XrM1iPVXlaH34wSj17v6Y33uxoOprLtWnTXGNGD9Kw4Z/qsRqNtHPXXv26ZJby5ctjOppTtv59Ss8/UUkz+rbTpB6tlZRs1ZsTftS1+MSUbcYsWKM//jqi0Z2bakrvNjofe0V9vlmU8nyy1aoeE39SYnKypr3zvIa93FCL/tyrLxdvMHFKaSqztfe98sTzjog4o/ffD1VISBPVrNlUv6/ZoAULpqhcudKmo7mcJ7a35Jnn7amfWxxhs9qMLfHx8bp06ZLdEh8f7/S5/P777woKClKZMmX05ptv6sKFCynPbdy4Ubly5VK1atVS1tWvX19eXl76888/7/kYdCjSwLJlqzVo0Cj9/PNS01HcqsbjVbRkyUotW7ZaJ05E6OefftOqsHWqWq2S6Wgu17tnV02eMlvTZ8zTvn1/661u7+nq1Wvq9Eo709Gc8mX3VmoRUkGlCuVVmQfzaehLDXTm4mXtPXFOknT5WrwWbtytd1o9oepliqp80fwa0qGBdh45o11Hz0iSNu47riNnovVRx0YqWyRItSsE661nQjTvj51KTEo2eXr3LbO1973yxPNe8utKLV22WocOH9Pfh45q0KBRiou7qho1HjUdzeU8sb0lzzxvT/3cklGEhoYqZ86cdktoaKhT+2rUqJFmzJihsLAwffzxx1qzZo0aN26s5OQb78tnz55VUFCQ3c/4+Pgod+7cOnv27D0fhw4FnPbnpm168j81VapUsCTp4UfKKqRmNa1YvsZwMtfy9fVVlSoVFbZqbco6m82msFXr9PjjVQ0mSztx1xIkSTkD/SVJ+06cU1KyVTXKFk3ZJrhAbhV8ILt2/n+HYtfRMypVKK/y5AhM2aZmueKKu56gw2cuKKPyhPa+FU8979S8vLzUpk1zBQZm1aZN20zHcSlPbW9PPW+kbwMGDFBsbKzdMmDAAKf21a5dOzVv3lyPPPKIWrZsqcWLFys8PFy///57mmY2ftnYffv2adOmTQoJCVHZsmW1f/9+ffbZZ4qPj1eHDh1Ut27dO/58fHz8TWUgm80mi8XiytiQ9OknE5U9RzZt2b5CycnJ8vb21tAhn2je3J9NR3OpvHlzy8fHR5HnouzWR0aeV9kyJQ2lSjtWq02jf/hdlUsUUqlCeSVJUZeuytfHWzkC/O22zZ0jIGUeRdSlK8qTI+Cm5/95LqPK7O19O5563pJUoUJZ/bHmJ/n7+yku7oratu2q/fv/Nh3LpTy1vT31vHEPDF421s/PT35+fi7Zd4kSJZQ3b14dOnRI9erVU4ECBRQZGWm3TVJSkqKjo1WgQIF73q/RCsXSpUtVuXJl9e3bV48++qiWLl2qJ554QocOHdLx48fVoEEDrVq16o77uFVZyGq97KYz8GytWjdV2+ebq3OnXqpTq7neeK2v3n67i15s38p0NNyH0LmrdOj0BX38ahPTUQAjDh48rOrVG6l2neb6+puZmjx5rMqWfch0LAC4b6dOndKFCxdUsGBBSVJISIhiYmK0devWlG1WrVolq9WqGjVq3PN+jXYohg4dqn79+unChQuaOnWqXnzxRXXt2lUrVqxQWFiY+vXrp5EjR95xH7cqC3l5ZXfTGXi2YSPe09hPvtIPCxZr754DmvP9T/ri82/V5503TUdzqaioaCUlJSkof1679UFB+XT23HlDqdJG6NxV+mP3EU3u+ZzyP/Dv31HeHAFKTErWpavX7baPvnQ1ZYhT3hyBunDp6k3P//NcRpWZ2/tOPPW8JSkxMVGHjxzT9u1/6YMPPtZff+1Vj+6vmo7lUp7a3p563rgHGeSysXFxcdqxY4d27NghSTp69Kh27NihEydOKC4uTv369dOmTZt07NgxhYWFqUWLFipVqpQaNrxxZa9y5cqpUaNG6tq1qzZv3qz169ere/fuateunQoVKnTPOYx2KPbs2aNXXnlFktS2bVtdvnxZzz33XMrz7du3165du+64Dz8/P+XIkcNuYbiTewRkzSqr1f43P9lqlZdX5p6ak5iYqG3bdqnuU7VT1lksFtV9qrY2bdp6h59Mv2w2m0LnrtKqnYf0dc/nVDhvTrvnyxXNLx9vL20+cDJl3bFz0Tpz8bIqBd/4lqNicEEdOh2l6Mv/dio27j+ubP5ZVKJAbveciAtkxva+F5563rdi8fJSFhcNP0gvPLW9PfW8kXls2bJFjz76qB599MaFI/r06aNHH31UH374oby9vbVr1y41b95cpUuXVufOnVW1alWtXbvWbkjVrFmzVLZsWdWrV09NmjRR7dq19fXXXzuUw/gcin8+/Ht5ecnf3185c/77QSZ79uyKjY01Fe2eBQYGpExMlqTg4kVVqVIFRUdf1MmT934N34zmt9/C1Lf/Wzp18rT27TuoipUqqHv3VzVz5gLT0Vxu7GffaOqUsdq6bZfCw7fr7R5dFRiYVdOmzzUdzSkfzV2l37Yc0LjXmyvQL4uiYm/MeciW1U/+WXyUPaufng15WJ/8sEY5A/0V6J9FI+etVsXggqr4/x2KkHLFVKJgbg2cvlS9WtbRhUtX9MWiDWr7RCVl8TX+UnNfMlt73ytPPO9hw97VsmW/6+TJCGXLlk3t2rXQk0+E6JlmHUxHczlPbG/JM8/bUz+3ZEb/+c9/ZLPdfr7HsmXL7rqP3Llza/bs2feVw+i7fPHixfX333+rZMkbE582btyookX/vYrMiRMnUsZ4pWdVq1ZS2Mp/P0SPGTNYkjRjxjx17tLbUCrX6/fOEL3/YR99Mm6o8uXLo7Nnzmnqt99rZOgE09Fcbv78X5Qvb24N/rCvChTIp50796jpMx0UGRl19x9Oh+avvVEJ7DJuvt36IR0aqEVIBUlS3+eelMXLone+WaSEpGTVLFdc/33+34smeHt5afwbLTViTpg6jpmjrH6+alajvN56pqb7TsRFMlt73ytPPO98+fJqypSxKlggSLGxl7V79z4906yDwsLW3v2HMzhPbG/JM8/bUz+3OMJmcFJ2RmSx3alb42KTJk1SkSJF1LRp01s+/9///leRkZGaPHmyQ/v1zVI4LeJlOFl9M3dJ/nauJjp/s5eM7PKvH5iOYET2JsNMR4AbeWfyIZS3k2x1cCA1MjRPHaidmBBhOsJtXWzzH2PHfmD+78aO7SyjFYo33njjjs9/9NFHbkoCAAAA/D/69A7xzK9+AAAAAKQJOhQAAAAAnJaxL70CAAAApDEmZTuGCgUAAAAAp1GhAAAAAFJjUrZDqFAAAAAAcBoVCgAAACAVGxUKh1ChAAAAAOA0OhQAAAAAnMaQJwAAACA1hjw5hAoFAAAAAKdRoQAAAABSYVK2Y6hQAAAAAHAaHQoAAAAATmPIEwAAAJAaQ54cQoUCAAAAgNOoUAAAAACpMCnbMVQoAAAAADiNCgUAAACQChUKx1ChAAAAAOA0OhQAAAAAnMaQJwAAACAVhjw5hgoFAAAAAKdRoQAAAABSs1lMJ8hQMmWHwmY6gCHXEuNNR4Ab5W0+0nQEIx7KVdh0BCP+jokwHcGIZCvjDpD5eernFmQeDHkCAAAA4LRMWaEAAAAAnMWkbMdQoQAAAADgNCoUAAAAQCo2K5OyHUGFAgAAAIDTqFAAAAAAqTCHwjFUKAAAAAA4jQ4FAAAAAKcx5AkAAABIxcadsh1ChQIAAACA06hQAAAAAKkwKdsxVCgAAAAAOI0OBQAAAACnMeQJAAAASIU7ZTuGCgUAAAAAp1GhAAAAAFKx2UwnyFioUAAAAABwGhUKAAAAIBXmUDiGCgUAAAAAp9GhAAAAAOA0hjwBAAAAqTDkyTFUKAAAAAA4jQoFAAAAkAqXjXUMFQoAAAAATkuTDkVMTExa7CbDe/ONjjp0cJPiLh3WhnWL9Fi1yqYjuVTt2jW0cOE0HT+2VYkJEWrevKHpSG7lae0tSQUL5deUKWN14uR2RV3Yr82bl+rRKo+YjpWmur7dUfOWTdOWI6u1bs9STZg+WsVLFrXbZvrCidoXudluGTT6PUOJXcsTf88lzpvzrmw6klt46nkj7Tncofj44481d+7clMdt27ZVnjx5VLhwYe3cuTNNw2Ukbdo015jRgzRs+Kd6rEYj7dy1V78umaV8+fKYjuYygYEB2rVrr97uOdB0FLfzxPbOlSuHwsJ+UGJSkp599hVVrVJfAwaMUMzFWNPR0tRjNato9rfz1a5xZ3Vu20O+Pt6aMm+Csgb42203b8ZC1Xm4ccoyZsgEQ4ldxxN/zyXOm/PmvHFjUrapJSOy2GyOjRILDg7WrFmzVLNmTa1YsUJt27bV3LlzNW/ePJ04cULLly93VdZ75pOlsNuPuWHdIoVv2amevd6XJFksFh07Eq4vvpyqUaO/cEsGk7+CiQkRav3cq/rll2VuP7aJYY7pob39fHzdcpx/DB36rh4PqaoGT7d163H/V9FsQW493gN5cmnDvuV6qfnr2rJpu6QbFYr9uw8q9IOxbsvxd0yE2471j/Twe24C5815c97uOe+kBPe/rt2rI480MHbsEn+Z/yztKIcrFGfPnlWRIkUkSYsXL1bbtm3VoEED9e/fX+Hh4fcdyMH+Tbrg6+urKlUqKmzV2pR1NptNYavW6fHHqxpMBlfw1PZu0rS+tm/7SzO/+0LHjm3Rho1L9EqndqZjuVz2HNkkSbEx9pWYZ1o30oZ9y/XLmu/Ve+Bb8s/qZyKey3jq7znnzXlz3pn3vB1hs1mMLRmRwx2KBx54QCdPnpQkLV26VPXr15d04xcxOTn5vgP5+flp3759970fd8qbN7d8fHwUeS7Kbn1k5HkVyJ/PUCq4iqe2d3BwUXXp2kGHDx9TixYd9c0332nMmMFq37616WguY7FYNGBYH239c4f+3n8kZf3iH5epf7dB6tjqTX09fpqat2msUV8ONZg07Xnq7znnzXlLnDfgKIcvG9uqVSu9+OKLeuihh3ThwgU1btxYkrR9+3aVKlXqnvfTp0+fW65PTk7WyJEjlSfPjTF8n3766R33Ex8fr/j4eLt1NptNFkvG7OEB6ZWXl0Xbtv2lwYNGS5J27tyj8uVLq3OX9po16wfD6Vzjw4/766GyJdS+2Wt26+fP/Cnl33/vO6zz5y5o2o9fqkjxwjp5LP2W8AEA98ZmNZ0gY3G4QzF27FgVL15cJ0+e1KhRo5Qt243hAGfOnNFbb711z/sZN26cKlWqpFy5ctmtt9ls2rdvnwIDA++pUxAaGqohQ4bYrbN4ZZPFO8c9Z7lfUVHRSkpKUlD+vHbrg4Ly6ey5827LAffw1PY+ezZS+/f/bbfuwIHDatmysaFErvV+aF89+XRtvdTidZ07E3nHbXdt2y1JKhpcJNN0KDz195zz5rwlzhtwlMNDnnx9fdW3b1999tlnevTRR1PW9+7dW126dLnn/Xz00UeKjY3VBx98oNWrV6cs3t7emjZtmlavXq1Vq1bddT8DBgxQbGys3WLxyu7oad2XxMREbdu2S3Wfqp2yzmKxqO5TtbVp01a3ZoHreWp7b9q4VQ89VMJu3UOlgnXiROb4AJ3a+6F9Vb/Jf9Sp1VuKOHH6rtuXfbi0JOn8/wwfyMg89fec8+a8Oe/Me95wnXuqUPzyyy/3vMPmzZvf03bvvfee6tWrpw4dOqhZs2YKDQ2Vr6/jV63x8/OTn5/9ZEgTw53GfvaNpk4Zq63bdik8fLve7tFVgYFZNW363Lv/cAYVGBigUqWCUx4HFy+qSpUqKDr6ok6evPuHsIzME9t7wudTtGrVD+rb7y39+MMSVatWSZ1efUE9ug8wHS1NffhxfzVt1VDdX+6rK1euKm/QjeGXly/FKf56vIoUL6xnWjXUmpUbFHMxVmXKl9J7w3orfMM2Hdx7yHD6tOWJv+cS5815c96QrBl0crQp99ShaNmy5T3tzGKxODQx+7HHHtPWrVvVrVs3VatWTbNmzcqwcx/mz/9F+fLm1uAP+6pAgXzauXOPmj7TQZGRmecby/9VtWolha1ckPJ4zJjBkqQZM+apc5fehlK5hye297atu9Su3esaOqS/BgzoqWPHTqp//6GaO/dn09HS1AudnpMkzfj5K7v1A3oM0U9zlygxIVEhT1TXy6+9oKwB/jp7+pxWLF6tiZ9+ayKuS3ni77nEeXPenDfgKIfvQ+Eqc+bMUa9evXT+/Hn99ddfKl++vNP7MnEfivQgY3bF7l+6+AU2wN33oUgv3H0fivTCxH0oAMCV0vN9KA6UNTc/sMz+34wd21kOT8pO7fr16/L397/7hvegXbt2ql27trZu3apixYqlyT4BAAAAuJbDk7KTk5M1bNgwFS5cWNmyZdORIzeuzf7BBx9oypQp9xXmwQcfVIsWLRQYGHhf+wEAAADgHg53KEaMGKFp06Zp1KhRypIlS8r6hx9+WJMnT07TcAAAAIC72awWY0tG5HCHYsaMGfr666/Vvn17eXt7p6yvVKmS9u/fn6bhAAAAAKRvDs+hiIiIuOUdsa1WqxITE9MkFAAAAGBK+rhkUcbhcIWifPnyWrt27U3rFyxYYHejOwAAAACZn8MVig8//FAdO3ZURESErFarfvzxRx04cEAzZszQ4sWLXZERAAAAcJuMOpfBFIcrFC1atNCiRYu0cuVKBQYG6sMPP9S+ffu0aNEiPf30067ICAAAACCdcuo+FHXq1NGKFSvSOgsAAACADMbpG9tt2bJF+/btk3RjXkXVqlXTLBQAAABgitXGkCdHONyhOHXqlF544QWtX79euXLlkiTFxMSoZs2amjNnjh588MG0zggAAAAgnXJ4DkWXLl2UmJioffv2KTo6WtHR0dq3b5+sVqu6dOniiowAAACA29hsFmNLRuRwhWLNmjXasGGDypQpk7KuTJkymjBhgurUqZOm4QAAAACkbw5XKIoUKXLLG9glJyerUKFCaRIKAAAAQMbgcIdi9OjR6tGjh7Zs2ZKybsuWLerZs6fGjBmTpuEAAAAAd7PZzC0ZkcVmu3v0Bx54QBbLv2O6rly5oqSkJPn43Bgx9c+/AwMDFR0d7bq098gnS2HTEYzImKPu7l8G/du7b34+vqYjGFE0W5DpCEb8HRNhOgIApKmkhPT7urareDNjx654bJGxYzvrnuZQjBs3zsUxAAAAgPSBy8Y65p46FB07dnR1DgAAAAAZkNM3tpOk69evKyEhwW5djhw57isQAAAAgIzD4Q7FlStX9O6772revHm6cOHCTc8nJyenSTAAAADAhIx6PwhTHL7KU//+/bVq1SpNnDhRfn5+mjx5soYMGaJChQppxowZrsgIAAAAIJ1yuEKxaNEizZgxQ//5z3/UqVMn1alTR6VKlVKxYsU0a9YstW/f3hU5AQAAALfIqJdvNcXhCkV0dLRKlCgh6cZ8iX8uE1u7dm398ccfaZsOAAAAQLrmcIeiRIkSOnr0qCSpbNmymjdvnqQblYtcuXKlaTgAAADA3aw2i7ElI3K4Q9GpUyft3LlTkvTee+/piy++kL+/v3r37q1+/fqleUAAAAAA6dc93Sn7To4fP66tW7eqVKlSqlixYlrlui/cKduzeOowR+6U7Vm4UzaAzCY93yl7y4MtjR272qmfjB3bWfd1HwpJKlasmIoVK5YWWXCfvL28TUcwIsnqmZcqTkhKNB3BCE/9YB39UnnTEYwoMf+46QhGFAnIZzqCEftjT5mOYERicpLpCPgfXDbWMffUoRg/fvw97/Dtt992OgwAAACAjOWeOhRjx469p51ZLBY6FAAAAMjQMurkaFPuqUPxz1WdAAAAACA1h6/yBAAAAAD/uO9J2QAAAEBm4qlXkHQWFQoAAAAATqNCAQAAAKTCpGzHUKEAAAAA4DSnOhRr165Vhw4dFBISooiIGzeZmjlzptatW5em4QAAAAB3s9ksxpaMyOEOxQ8//KCGDRsqa9as2r59u+Lj4yVJsbGx+uijj9I8IAAAAID0y+EOxfDhwzVp0iR988038vX1TVlfq1Ytbdu2LU3DAQAAAEjfHJ6UfeDAAT3xxBM3rc+ZM6diYmLSIhMAAABgjNV0gAzG4QpFgQIFdOjQoZvWr1u3TiVKlEiTUAAAAAAyBoc7FF27dlXPnj31559/ymKx6PTp05o1a5b69u2rN9980xUZAQAAALexyWJsyYgcHvL03nvvyWq1ql69erp69aqeeOIJ+fn5qW/fvurRo4crMgIAAABIpxzuUFgsFg0cOFD9+vXToUOHFBcXp/LlyytbtmyuyAcAAAAgHXP6TtlZsmRR+fLl0zILAAAAYJzVZjpBxuJwh+Kpp56SxXL78V2rVq26r0AAAAAAMg6HOxSVK1e2e5yYmKgdO3Zo9+7d6tixY1rlAgAAAIywZtDJ0aY43KEYO3bsLdcPHjxYcXFx9x0IAAAAQMbh8GVjb6dDhw769ttv02p3AAAAgBFcNtYxadah2Lhxo/z9/dNqdwAAAAAyAIeHPLVq1crusc1m05kzZ7RlyxZ98MEHaRYMAAAAQPrncIciZ86cdo+9vLxUpkwZDR06VA0aNEizYAAAAIAJVtMBMhiHhjwlJyerU6dO+vTTTzV16lRNnTpVU6ZM0ciRI+lMSHrzjY46dHCT4i4d1oZ1i/RYtcqmI7lV375v6fr1Exo9epDpKG7hae1du3YNLVw4TcePbVViQoSaN29oOpJbZbb29i79iLL2GKpsn8xRjikr5PNoTbvn/V/tpxxTVtgtAb0+stsma4+hyjZqlrJPWqJsn8yRf5d3ZcmVx52ncd9CalbTrLmTtPvAWkVdOqjGTevbPd+0WQPN/+lbHTz2p6IuHdTDj5QzlDTtvNrjJX23dLLWHVqhsN2L9enUUBUrWdRumweLFdYn336kVXsWa+3fy/Xx10OVO+8DhhK7zsCBvXTt2nG7ZceOMNOx3Cazva7BHIc6FN7e3mrQoIFiYmJcFCfjatOmucaMHqRhwz/VYzUaaeeuvfp1ySzly5ex3lydVbVqRXXp8qJ27dprOopbeGJ7BwYGaNeuvXq750DTUdwuM7a3JYu/rKeO6Pp3E267TdJfm3W5d9uU5erX9h2K5P07dG3ScMUN7KRrXw6VV76Cyvpmxhr6GhAYoN2796v/O0Nv83xW/blxq4Z+OMbNyVynSkhlzZ36o15u+prebNtLPr4+mjh3rPwDbsyD9A/w15dzx8pmk15r/bY6NXtDvr6++mzmqDvehyqj2rPngIoXr5ay1Kv3nOlIbpEZX9fSEpOyHePwpOyHH35YR44ccUWWDK13z66aPGW2ps+Yp337/tZb3d7T1avX1OmVdqajuVxgYICmTRuvt956TzExsabjuIUntveyZas1aNAo/fzzUtNR3C4ztnfS7nDFL5ympO3rb7uNLSlRtksXUxZdtb80eMKKH5V8ZJ9sFyKVfHivEn6dK+8S5SRvb1fHTzNhK/5Q6LBx+nXxils+P3/Ozxrz8Rda8/sGNydzne4vvqNFc3/VkQNHdXDvIQ3qOUIFHyyg8hXLSJIqP1ZRhYoU0KCew3Vo/xEd2n9EH749XOUrlVX12lUNp097SUlJOnfufMpy4cJF05HcIjO+rsEchzsUw4cPV9++fbV48WKdOXNGly5dsls8ka+vr6pUqaiwVWtT1tlsNoWtWqfHH898L77/67PPhuu331Zp1ap1pqO4hae3t6fx5Pb2KVNJ2cbOU+CIb+Xf4W1ZArPffuPA7PJ9vK6SD++VkpPdFxL3LVv2QElSbMyN9/AsWXxls9mUkJCYsk18fIKsVqsq16hoJKMrlSoVrCNHNmvv3rWaOvUzFSlSyHQkl/Pk1zW4xj1Pyh46dKjeeecdNWnSRJLUvHlzu9KnzWaTxWJR8n28kVy5ckXz5s3ToUOHVLBgQb3wwgvKkyf9l97y5s0tHx8fRZ6LslsfGXleZcuUNJTKPdq0aabKlR9WrVrNTEdxG09ub0/kqe2dtDtcSVvXyRp1Rl5BheTX6lUF9PpIVz7qKdn+na7o91wXZanbXBa/rEo6vFfXPnvfYGo4ymKxqO+wntr+504d3n9UkvTXtj26dvW6er7/lj4PnSRZLOo58E35+Pgob1D6f092RHj4Dr322js6ePCIChQI0sCBvbRy5XxVrdpAcXFXTMdzGU99XXMEk7Idc88diiFDhuiNN97Q6tWr0+zg5cuX17p165Q7d26dPHlSTzzxhC5evKjSpUvr8OHDGjZsmDZt2qTg4ODb7iM+Pl7x8fF26/7p3MC1HnywoMaMGaymTdvf1AYAMrakzb+n/NsacUzJJ48o+8cz5V22kpL3bU95LmHpPCWu/U2WPPnl1/wl+Xd5l05FBjJg5DsqVbaEOjV/M2XdxQsx6t/1A/334756octzslqtWrpwpfbu3C+bzWYwbdpbvvz3lH/v3r1f4eE7dODAerVu/YymT59rLhiQwdxzh+KfF5Enn3wyzQ6+f/9+JSUlSZIGDBigQoUKaceOHcqZM6fi4uL07LPPauDAgZo9e/Zt9xEaGqohQ4bYrbN4ZZPFO0ea5bybqKhoJSUlKSh/Xrv1QUH5dPbcebflcLdHH31E+fPn06ZNv6as8/HxUe3aNfTmmx2VI0cpWa2Zr4/vqe3tqWjvG2xRZ2W9HCOvoEJ2HQpb3CXZ4i5J5yJ07cwJZR/zvRJKllPy4X0G0+JevPtRH9WpX1Odn+2myDP2v8ub1mxW88fbKlfunEpKSlbcpTit2PWLlv2cua+AFBt7SYcOHVXJksVMR3EpXtfuLvN9enEth+ZQuPJb/40bN2rw4MEp97nIli2bhgwZonXr7jwuf8CAAYqNjbVbLF53GOfrAomJidq2bZfqPlU7ZZ3FYlHdp2pr06atbs3iTqtXr1eVKvVVvXqjlGXLlp2aM+cnVa/eKFN2JiTPbW9PRXvfYHkgryyBOWSLib7DRv//HuHj655QcNq7H/VR3cZP6PXn3tbpE2duu11MdKziLsXpsVpVlDvvA1qzLHPPlQsMDFBwcDGdPRtpOopL8bqGtObQje1Kly59105FdPQd3mxu4Z/9Xb9+XQULFrR7rnDhwjp//s49ZT8/P/n5+d1yn+409rNvNHXKWG3dtkvh4dv1do+uCgzMqmmZuGQaF3dFe/cetFt39epVXbhw8ab1mY0ntndgYIBKlfp3+GFw8aKqVKmCoqMv6uTJ0waTuV6mbG8/f3kFFU556JW3gLyKlJTtyiXZrlyWX/OXbsyhiI2WV1Ah+T/XRdbI00ras0WS5B1cVl7BZZT8927Zrl6WV75C8mv5iqznIjJUdSIwMEDBJf79NrpY8Qf18CPldPFijCJOnVGuB3LqwQcLqUDBIElSqYdu/A1EnjuvyMioW+4zvRsw8h01fvZp9X7lPV2Ju6o8+XJLkuIuxyn+eoIkqXm7Jjp68LguXohRxWoV1G9YL836eq6OHz5hMnqaCw0dqCVLVurEiQgVKpRf77/fW8nJyZo37xfT0VwuU76upaGMevlWUxzqUAwZMuSmO2Xfr3r16snHx0eXLl3SgQMH9PDDD6c8d/z48QwxKVuS5s//Rfny5tbgD/uqQIF82rlzj5o+0yHDvuHgzjyxvatWraSwlQtSHo8ZM1iSNGPGPHXu0ttQKvfIjO3tXby0Avt/kvLYv92NMfQJ65fr+szP5P1gCfnWfFqWgGyyxVxQ0p6tiv9pmpR048o/toTr8q1SS34tXpbFz//GNru36NriWSnbZASVH31YP//6Xcrj4aH/lSR9P+tH9XjzPTVqXFefT/o45fnJ08ZJkkaFTtCo0NvfwyM9a/tKK0nS5IVf2K3/sOcILZp7Ywhr8ZJF1eO/byhnrhw6ffKMpnw2Xd99lfk+aBYuXEAzZkxQ7ty5FBUVrQ0bwvXkky0VFeXYl6MZUWZ8XYM5Fts9zrDy8vLS2bNnFRQUlGYH/9+5D48//rgaNvz37rv9+vXTqVOn9P333zu0X58she++USbk45Vxrv2elpKsnnmJSk/97iRzTQm9d9EvlTcdwYgS84+bjmBEkYB8piMYsT/2lOkIRiQmJ5mOYERSQoTpCLe1JP8Lxo7d9Ny9f+79448/NHr0aG3dulVnzpzRwoUL1bJly5TnbTabBg0apG+++UYxMTGqVauWJk6cqIceeihlm+joaPXo0UOLFi2Sl5eXWrdurc8++0zZsmW75xz3XKFwxTCiQYMG3fH50aNHp/kxAQAAgDuxZpBv7a5cuaJKlSrp1VdfVatWrW56ftSoURo/frymT5+u4OBgffDBB2rYsKH27t0rf39/SVL79u115swZrVixQomJierUqZNee+21O14U6X85fJUnAAAAAOY1btxYjRs3vuVzNptN48aN0/vvv68WLVpIkmbMmKH8+fPrp59+Urt27bRv3z4tXbpU4eHhqlatmiRpwoQJatKkicaMGaNChe7tRo/3fJUnq9WapsOdAAAAgPTIKouxJT4+XpcuXbJbnLnf19GjR3X27FnVr18/ZV3OnDlVo0YNbdy4UdKNq6zmypUrpTMhSfXr15eXl5f+/PPPez6WQ5eNBQAAAOA6oaGhypkzp90SGhrq8H7Onj0rScqfP7/d+vz586c8d6v50T4+PsqdO3fKNvfCoas8AQAAAHCdAQMGqE+fPnbr/vcWCekNHQoAAAAgFZMzh291jzVnFChQQJJ07tw5u3u9nTt3TpUrV07ZJjLS/kaOSUlJio6OTvn5e8GQJwAAACCTCQ4OVoECBRQWFpay7tKlS/rzzz8VEhIiSQoJCVFMTIy2bv33DumrVq2S1WpVjRo17vlYVCgAAACAVKymA9yjuLg4HTp0KOXx0aNHtWPHDuXOnVtFixZVr169NHz4cD300EMpl40tVKhQyr0qypUrp0aNGqlr166aNGmSEhMT1b17d7Vr1+6er/Ak0aEAAAAAMqQtW7boqaeeSnn8z9yLjh07atq0aerfv7+uXLmi1157TTExMapdu7aWLl2acg8KSZo1a5a6d++uevXqpdzYbvz48Q7luOc7ZWck3Cnbs3CnbM+S6V6w7hF3yvYs3Cnbs3Cn7PRnQcH2xo793JlZxo7tLOZQAAAAAHAaHQoAAAAATmMOBQAAAJCKpw6vdRYVCgAAAABOo0IBAAAApJJRLhubXlChAAAAAOA0OhQAAAAAnMaQJwAAACAVq6fe7MlJVCgAAAAAOI0KBQAAAJCKVZQoHEGFAgAAAIDTqFAAAAAAqXBjO8dQoQAAAADgNDoUAAAAAJzGkCcAAAAgFS4b6xg6FJlIsjXZdAQjPPVvPouPr+kIRiQkJZqOYETumXtNRzAipntV0xGMqDTjlOkIRnjq+xiQ0dGhAAAAAFKxmg6QwTCHAgAAAIDT6FAAAAAAcBpDngAAAIBUuA+FY6hQAAAAAHAaFQoAAAAgFS4b6xgqFAAAAACcRocCAAAAgNMY8gQAAACkwn0oHEOFAgAAAIDTqFAAAAAAqVChcAwVCgAAAABOo0IBAAAApGLjsrEOoUIBAAAAwGl0KAAAAAA4jSFPAAAAQCpMynYMFQoAAAAATqNCAQAAAKRChcIxVCgAAAAAOI0OBQAAAACnMeQJAAAASMVmOkAGQ4UCAAAAgNOoUAAAAACpWLlTtkOoUAAAAABwGh2KNPTmGx116OAmxV06rA3rFumxapVNR3Kp2rVraOHCaTp+bKsSEyLUvHlD05HcwlPPW5IKFsqvKVPG6sTJ7Yq6sF+bNy/Vo1UeMR3LpTy5vaXM97rmVaKC/Du/r4BBU5Xt01/k/XCNm7axBD0o/1cHKnDE9woMnaesvT6RJVfelOd9Hm+orG+NUOBHc5Tt018k/0B3nsJ9e6NnJy1cMVM7j63V5n0rNWnGJwouVcxum3Yvt9Ksn7/WjqN/6HDUNmXPkc1QWtfq36+bNqxfrAtR+3Xq5A4tmD9ZpUuXMB3LbTLb33dashpcMiI6FGmkTZvmGjN6kIYN/1SP1Wiknbv26tcls5QvXx7T0VwmMDBAu3bt1ds9B5qO4laeet65cuVQWNgPSkxK0rPPvqKqVeprwIARirkYazqaS3lqe0uZ83XNksVP1tNHFf/jV7d+Pk8BBfQYKWtkhK59OVBXx7ythBVzpaREu30k7d+mhJXz3RU7TdWoWVXfTZmn5xp21MvPvSkfXx9Nn/+lsgb4p2zjn9Vff4Rt0MSx3xpM6np1ngjRxEnTVadOczVp8oJ8fH21ZPFsBQRkNR3N5TLj3zfMsdhstkw3kd0nS2G3H3PDukUK37JTPXu9L0myWCw6diRcX3w5VaNGf+GWDCaH+yUmRKj1c6/ql1+WGUzhfibPO4uPr1uPN3Tou3o8pKoaPN3Wrcf9XwmpPti5m8n2NvFCnR5e12K6V3XZvrN9+ouufTtCybv/TFnn91JfKTlZ8bPH3vXnvUs+rKzdPlLcf1+Qrl9J02yVZpxK0/3dSe48uRR+YJXaNeui8I3b7J6rUauqZv/8jSqXeEKXL8W5PMvJy5EuP8ad5M2bW6cjdqluvdZat+7Pu/9AGrEa+CiWHv6+kxIi3HIcZ4wt2sHYsXuf+M7YsZ1FhSIN+Pr6qkqVigpbtTZlnc1mU9iqdXr8cde9GQLu1KRpfW3f9pdmfveFjh3bog0bl+iVTu1Mx4KLeOTrmsUin3LVZD1/Wv6vDVbAkBnK2nP0LYdFZSbZc2SXJMVm8mrjvciZM4ck6WJ0jNkgLuaRf98OYsiTY+hQpIG8eXPLx8dHkeei7NZHRp5Xgfz5DKUC0lZwcFF16dpBhw8fU4sWHfXNN99pzJjBat++telocAFPfF2zZMspi3+AstRtreT923T9q0FK+muT/F8ZIK+SFUzHcwmLxaL3R/TVlk3bdXD/YdNxjLJYLBozZrDWr9+sPXsPmI7jUp749w3XMnrZ2G3btumBBx5QcHCwJGnmzJmaNGmSTpw4oWLFiql79+5q1+7O34DGx8crPj7ebp3NZpPFwvW+gLTk5WXRtm1/afCg0ZKknTv3qHz50urcpb1mzfrBcDogDVhufMeWtOdPJf7xiyTJevqovIuXlW9IY8Uf3mMynUsMGfWeSpctqeebvmo6inHjx49QhfJl9FTdVqajIB3IdPMBXMxohaJTp046fPjGNyKTJ0/W66+/rmrVqmngwIF67LHH1LVrV3377Z0nhIWGhipnzpx2i8162R3xU0RFRSspKUlB+fParQ8Kyqez5867NQvgKmfPRmr//r/t1h04cFhFihQylAiu5Imva7Yrl2RLTpL17Em79dbIU/J6IPN9azto5Luq26CO2rd8TWfPmJ27YNq4ccPVpHF9NWjYVhERZ0zHcTlP/PuGaxntUPz999966KGHJElffvmlPvvsM3322Wd64403NHbsWH311Vf65JNP7riPAQMGKDY21m6xeGV3R/wUiYmJ2rZtl+o+VTtlncViUd2namvTpq1uzQK4yqaNW/XQQ/aXU3yoVLBOnEi/k+rgPI98XUtOkvXE3/IKsr+wh1e+QrJezFwfuAeNfFcNmj6lDs++rlMnTpuOY9S4ccPVonkjNWz0vI4dO3n3H8gEPPLvGy5ldMhTQECAoqKiVKxYMUVERKh69ep2z9eoUUNHjx694z78/Pzk5+dnt87EcKexn32jqVPGauu2XQoP3663e3RVYGBWTZs+1+1Z3CUwMEClSgWnPA4uXlSVKlVQdPRFnTyZed+gPPW8J3w+RatW/aC+/d7Sjz8sUbVqldTp1RfUo/sA09FcylPbW8qkr2tZ/OWVt2DKQ6/c+WUrFCzb1cuyxUQp4feF8n+pn5KP7FHyob/kU7aKvMtX17Uv/5vyM5bsuWTJ/oAs/78fr4LFpPhrssacl666/kpI92vIqPfUvHVjvf5Sb8XFXVXeoBuXCb18KU7x128MIc4blEf5gvKoWHARSVKZ8g/pStwVnT51VrExl4xlT2vjx49Qu+dbqvVznXX5cpzy///8gdjYy7p+/brhdK6VKf++0xB3ynaM0cvGvvTSS/Lz89PkyZPVtm1blSlTRsOGDUt5PjQ0VN9//7127drl0H5NXDZWkt568xW90+dNFSiQTzt37lGv3h9qc/h2tx3f3b/7TzwRorCVC25aP2PGPHXu0tvNadwnvZy3uy8bK0mNGtfV0CH9VbJUsI4dO6kJEyZr2tQ5bs3g7svGppf2NvVCbfp1La0vG/vPpV7/V+LmMMXP+UyS5FO9vrLUe06WXHlkjYxQwtLvlbzn30uIZmn4grI0fOGmfVz/fpySwlelSU5XXjb2cNS2W67v332QfpizSJL0dv/X1bP/63fcxhXcfdnYhPhb/3fu3KW3Zs50331GTFw2VjL/952eLxs7qpi5y8b2P57xLhtrtENx+vRp1apVS0WLFlW1atU0ceJEVa1aVeXKldOBAwe0adMmLVy4UE2aNHFov6Y6FKbRmfYsJjoU6YHJ+1CY5KkTBF15H4r0zJ33oUhPTN+HwhRTHQrT0nOHYqTBDsV7GbBDYXQORaFChbR9+3aFhIRo6dKlstls2rx5s5YvX64HH3xQ69evd7gzAQAAAMB9jM6hkKRcuXJp5MiRGjlypOkoAAAAgMdWhZ3Fje0AAAAAOI0OBQAAAACnGR/yBAAAAKQnVgY9OYQKBQAAAACnUaEAAAAAUrGaDpDBUKEAAAAA4DQ6FAAAAACcxpAnAAAAIBWmZDuGCgUAAAAAp1GhAAAAAFJhUrZjqFAAAAAAcBoVCgAAACAVq8V0goyFCgUAAAAAp9GhAAAAAOA0hjwBAAAAqVi5cKxDqFAAAAAAcBoVCgAAACAV6hOOoUIBAAAAwGl0KAAAAAA4jSFPAAAAQCrcKdsxVCgAAAAAOI0KBQAAAJAKl411DBUKAAAAAE6jQgEAAACkQn3CMZmyQxHg62c6ghEJyUmmIxiRbE02HcGIRA9tb0/lZbGYjmDEf76/aDqCEbsntTIdwYj8Hb81HcGI+ORE0xGA+8KQJwAAAABOy5QVCgAAAMBZXDbWMVQoAAAAADiNCgUAAACQCpeNdQwVCgAAAABOo0MBAAAAwGkMeQIAAABSYcCTY6hQAAAAAHAaFQoAAAAgFS4b6xgqFAAAAACcRoUCAAAASMXGLAqHUKEAAAAA4DQ6FAAAAACcxpAnAAAAIBUmZTuGCgUAAAAAp1GhAAAAAFKxMinbIVQoAAAAADiNDgUAAAAApzHkCQAAAEiFAU+OoUIBAAAAwGlUKAAAAIBUmJTtGCoUAAAAAJxGhwIAAACA0xjyBAAAAKTCnbIdQ4UiDXh5een9D3pr1541Ohe1Vzv/Wq3+73Y3Hcvt+vZ9S9evn9Do0YNMR3Gp2rVraOHCaTp+bKsSEyLUvHlD05Hcon+/btqwfrEuRO3XqZM7tGD+ZJUuXcJ0LJejvTN3e7d+uaW+D5um3w8u1e8Hl+rbRRNVs26NlOef7dBMX/0wXr8fXKotZ9YqW45sBtM6b+vRc3p7xio9PXKBKg+cqVV7T9g9fyHumj5YsF5Pj1ygxwfP1lvTwnQ86pLdNgs2H1TnyctVa+gcVR44U5euJbjzFFzCU9+/X+v6kraEL9f5yL06H7lXa37/SQ0b/Md0LGRgdCjSQO8+b6hzl/bq12ewHqvytD78YJR69n5Nb7zZ0XQ0t6lataK6dHlRu3btNR3F5QIDA7Rr11693XOg6ShuVeeJEE2cNF116jRXkyYvyMfXV0sWz1ZAQFbT0VyK9s7c7R15JlKfj5iklxp20cuNumrL+m36ZGqoSpQuLknyz+qvDav/1NTxM80GvU/XEpJUuuADGtCs+k3P2Ww29f7ud0VcjNPYDv/RnG5NVTBXoN6YulLXEhJTtruemKxaDxVS5ycfdmd0l/LU9++IiDN6//1QhYQ0Uc2aTfX7mg1asGCKypUrbTpaumEz+L+MiCFPaaDG41W0ZMlKLVu2WpJ04kSEnmvTTFWrVTKczD0CAwM0bdp4vfXWe3rvvR6m47jcsmWrU9rakzRr1sHucZcuvXU6YpeqVKmodev+NJTK9WjvGzJre69dscHu8Zcjv1Hrl1vqkaoVdOTgMX3/zXxJUtWQygbSpZ3aZQqrdpnCt3zuxIXL2nUySgvebqZS+XNJkgY2r6F6I+frt53H1OqxhyRJHWqVkySFHznrlszu4Knv30t+XWn3eNCgUXqt60uqUeNR7dt30FAqZGRUKNLAn5u26cn/1FSpUsGSpIcfKauQmtW0Yvkaw8nc47PPhuu331Zp1ap1pqPAjXLmzCFJuhgdYzYI3MIT2tvLy0sNWtRT1gB/7dq6x3Qct0lISpYk+fl4p6zz8rIoi4+3th+PNBXLLTz9/Vu68Xvfpk1zBQZm1aZN20zHSTesBpeMyGiFokePHmrbtq3q1KljMsZ9+/STicqeI5u2bF+h5ORkeXt7a+iQTzRv7s+mo7lcmzbNVLnyw6pVq5npKHAji8WiMWMGa/36zdqz94DpOHCxzN7eJcuW0NTFE5XFL4uuXbmmfq8O1NGDx0zHcpvi+XKqYK5AjV++XR+0rKGsvj76bsM+nYu9qqjL10zHcylPfv+uUKGs/ljzk/z9/RQXd0Vt23bV/v1/m46FDMpoh+KLL77Ql19+qZIlS6pz587q2LGjChQo4NA+4uPjFR8fb7fOZrPJYrGkZdQ7atW6qdo+31ydO/XSvn1/q2LFchr58Qc6e+acZs/60W053O3BBwtqzJjBatq0/U1tgMxt/PgRqlC+jJ6q28p0FLhBZm/v44dP6MX6rypbjkDVe+YpDR4/UK+16uExnQpfby998uKTGvzjRj0xfJ68vSyqUbKgapUupAw6nPueeer7tyQdPHhY1as3Uo6c2dWqVRNNnjxW9Z9uQ6cCTjE+h2L58uVatGiRxowZow8++ECNGzdW165d1aRJE3l53X1EVmhoqIYMGWK3LotPLvllecBVkW8ybMR7GvvJV/phwWJJ0t49B1SkSGH1eefNTP2C9Oijjyh//nzatOnXlHU+Pj6qXbuG3nyzo3LkKCWrNaMW73A748YNV5PG9VWvfmtFRJwxHQcu5gntnZSYpFPHIiRJ+3cdVPlKZfVCl+f0Uf8xhpO5T/nCeTSvxzO6fD1BiclW5Q70V4eJv6p84Tymo7mUp75/S1JiYqIOHzkmSdq+/S9Vq1pJPbq/qm7dB5gNlk5k1MnRphifQ/HII49o3LhxOn36tL777jvFx8erZcuWKlKkiAYOHKhDhw7d8ecHDBig2NhYuyWLby73hP9/AVmz3vTBOdlqvacOUUa2evV6ValSX9WrN0pZtmzZqTlzflL16o3oTGRC48YNV4vmjdSw0fM6duyk6ThwMU9tby8vi3yzZDEdw4js/lmUO9Bfx6MuaW9EtP5TrojpSC7lqe/ft2Lx8lIWPz/TMZBBGa9Q/MPX11dt27ZV27ZtdeLECX377beaNm2aRo4cqeTk5Nv+nJ+fn/z+5w/AncOdJOm338LUt/9bOnXytPbtO6iKlSqoe/dXNXPmArfmcLe4uCvau9f+ahBXr17VhQsXb1qfmQQGBqRM4JOk4OJFValSBUVHX9TJk6cNJnOt8eNHqN3zLdX6uc66fDlO+fPnkyTFxl7W9evXDadzHdo7c7d3t/++rg2rNunsqXMKyBagRq2eVtWaj6rHC+9IkvLky608Qbn1YPCDkqRS5UroatxVnY04p0sxl01Gd8jV+ESduPBv3oiLcdp/Olo5A/xUMFeglv91XA8E3vj332djNGpJuJ4qX0Q1HyqU8jNRl68p6vI1nfz//Rw6d1EBWXxVMFegcgZkzA+invr+PWzYu1q27HedPBmhbNmyqV27FnryiRA98z9Xd/NkfCXqGIvNZjNW0/Hy8tLZs2cVFBR0y+dtNptWrlypp59+2qH95gh0782XsmUL1Psf9tEzzRooX748OnvmnBbMX6SRoROUmJh49x2kkYTkJLcd63aWL5+rnTv3ql+/IXffOI0kW2/f4XSFJ54IUdjKm99sZsyYp85dersth7s7zgnxp265vnOX3po5c77bcrj7JYv2tufu9q6YO/juG92HDz55V4/Vqaq8QXkUd/mK/t57WDO+mKU//9giSXrtnU56re+rN/3c4J4fafG831yWa+2EtL2BYviRs+o6ZcVN65s9WkLDnqul2Rv2afq6vboQd135smfVM5VL6LWnHpFvqis/TQzbqa9W7bppH0Na11SLKiXTJGf+jt+myX7uVXp5/45Pdt+xJGnSpNF66qlaKlggSLGxl7V79z6N+WSiwsLWujVH/PX0W/nsWLy1sWNPP/aDsWM7y2iHIjg4WFu2bFGePGk7RtPdHYr0Ij10KExwd4civXD3B8z0wuBLllGe2t6u7lCkV2ndocgo3N2hSC/c3aFIL+hQ3FpG7FAYHfJ09OhRk4cHAAAAbmL10C+vnOV5s44AAAAApJl0MykbAAAASA+oTziGCgUAAAAAp1GhAAAAAFKxUqNwCBUKAAAAAE6jQwEAAADAaQx5AgAAAFKxMeTJIVQoAAAAADiNCgUAAACQitV0gAyGCgUAAACQAQ0ePFgWi8VuKVu2bMrz169fV7du3ZQnTx5ly5ZNrVu31rlz59I8Bx0KAAAAIIOqUKGCzpw5k7KsW7cu5bnevXtr0aJFmj9/vtasWaPTp0+rVatWaZ6BIU8AAABAKhnpPhQ+Pj4qUKDATetjY2M1ZcoUzZ49W3Xr1pUkTZ06VeXKldOmTZv0+OOPp1kGKhQAAABAOhEfH69Lly7ZLfHx8bfd/u+//1ahQoVUokQJtW/fXidOnJAkbd26VYmJiapfv37KtmXLllXRokW1cePGNM1MhwIAAABIxWbwf6GhocqZM6fdEhoaesucNWrU0LRp07R06VJNnDhRR48eVZ06dXT58mWdPXtWWbJkUa5cuex+Jn/+/Dp79mya/vdiyBMAAACQTgwYMEB9+vSxW+fn53fLbRs3bpzy74oVK6pGjRoqVqyY5s2bp6xZs7o0Z2p0KAAAAIBUTF421s/P77YdiLvJlSuXSpcurUOHDunpp59WQkKCYmJi7KoU586du+Wci/vBkCcAAAAgE4iLi9Phw4dVsGBBVa1aVb6+vgoLC0t5/sCBAzpx4oRCQkLS9LhUKAAAAIAMqG/fvmrWrJmKFSum06dPa9CgQfL29tYLL7ygnDlzqnPnzurTp49y586tHDlyqEePHgoJCUnTKzxJdCgAAAAAOzZbxrhs7KlTp/TCCy/owoULypcvn2rXrq1NmzYpX758kqSxY8fKy8tLrVu3Vnx8vBo2bKgvv/wyzXPQoQAAAAAyoDlz5tzxeX9/f33xxRf64osvXJqDDgUAAACQSka6sV16wKRsAAAAAE6jQwEAAADAaQx5AgAAAFIxeR+KjIgKBQAAAACnZcoKxbXEeNMR4EaeOm0qo1zSLq15WSymIxhh9dD23nnhiOkIRmR/caLpCEZE1CplOoIRxTYeNR0B/8PmsZ8unEOFAgAAAIDTMmWFAgAAAHAWl411DBUKAAAAAE6jQwEAAADAaQx5AgAAAFLx1AufOIsKBQAAAACnUaEAAAAAUuHGdo6hQgEAAADAaXQoAAAAADiNIU8AAABAKtwp2zFUKAAAAAA4jQoFAAAAkAp3ynYMFQoAAAAATqNCAQAAAKTCje0cQ4UCAAAAgNPoUAAAAABwGkOeAAAAgFSYlO0YKhQAAAAAnEaFAgAAAEiFG9s5hgoFAAAAAKfRoQAAAADgNIY8AQAAAKlYuQ+FQ6hQAAAAAHAaFQoAAAAgFeoTjqFCkQZq166hhQun6fixrUpMiFDz5g1NR3ILTz3vf7z5RkcdOrhJcZcOa8O6RXqsWmXTkdzC0867f79u2rB+sS5E7depkzu0YP5klS5dwnQst/G09uZ1LXO1t2/Fiso1IlR55/+g/KvXyK9WbbvnAzu+ojzTZyjo16XK98ti5RrziXzKlbPbxvvBB5Vz+Ajl++ln5Vv8qx4YP0G+lR9152m4XN++b+n69RMaPXqQ6SjIoOhQpIHAwADt2rVXb/ccaDqKW3nqeUtSmzbNNWb0IA0b/qkeq9FIO3ft1a9LZilfvjymo7mUJ553nSdCNHHSdNWp01xNmrwgH19fLVk8WwEBWU1HczlPbG9e1zJXe1v8syrx8CFd/mzcLZ9PPnVKlz/7TBc6d1L0291lPXtWD4waI0vOnCnb5PpopCze3rrYp7eiX++qpMOH9cBHofJ6ILebzsK1qlatqC5dXtSuXXtNR0lXrLIZWzIii82W+Wad+GYpbOzYiQkRav3cq/rll2XGMphg8rxN/AJvWLdI4Vt2qmev9yVJFotFx46E64svp2rU6C8MJHKP9HDeXhaLW45zO3nz5tbpiF2qW6+11q37023HNTFBMD20t8nW5nXN/e0dUauUy/adf/Uaxbw/UPHr1912G0tAgIKW/KaL7/RWwrZtsuTIqaCff1H02z2U+NeuG9tkzaqgX5fq4jt9lLBta5pkK7bxaJrsx1GBgQHatOlX9ez5vt57r4d27tyrfv2GuO3416+fcNuxHFWrcF1jx14fscrYsZ1FhQJwkK+vr6pUqaiwVWtT1tlsNoWtWqfHH69qMJlreep5/6+cOXNIki5Gx5gN4mK0t2ehvSX5+CjrM81kjbusxEOHJUm2S7FKOnFc/g0aSv7+kpe3sjZrruToaCUePGA48P377LPh+u23VVq16vadLOBeGJ+U/fnnn2vz5s1q0qSJ2rVrp5kzZyo0NFRWq1WtWrXS0KFD5eNz+5jx8fGKj4+3W2ez2WQx/C0mMq+8eXPLx8dHkeei7NZHRp5X2TIlDaVyPU8979QsFovGjBms9es3a8/ejP9h4k5ob8/iye2d5fEQ5fzwQ1n8/GW9cEEX+/aV7VJsyvMX33lHuYYPV9CS3ySbVdaLMYp5t79scXEGU9+/Nm2aqXLlh1WrVjPTUdKljDr0yBSjHYrhw4dr1KhRatCggXr37q3jx49r9OjR6t27t7y8vDR27Fj5+vpqyJDbl99CQ0Nvet7ilU3e3jlcHR+Ahxk/foQqlC+jp+q2Mh0FQBpJ2LFd0V26yCtnTmV95hnlGjRYF956Q7aYGElS9l69ZL0Yo4s9e8gWH6+sTZ9Rro8+UvQbr8saHW02vJMefLCgxowZrKZN29/0pSzgDKMdimnTpmnatGlq1aqVdu7cqapVq2r69Olq3769JKls2bLq37//HTsUAwYMUJ8+fezW5c5T1qW54dmioqKVlJSkoPx57dYHBeXT2XPnDaVyPU8973+MGzdcTRrXV736rRURccZ0HJfz9Pb2NB7d3tevK/l0hJJPRyhx317lmTlLWZs01dXZs5SlShX5PR6i882fke3qVUnS5XFjlaVqNfk3bKSr3882HN45jz76iPLnz6dNm35NWefj46PatWvozTc7KkeOUrJarQYTmpcJpxi7lNE5FKdPn1a1atUkSZUqVZKXl5cqV66c8nyVKlV0+vTpO+7Dz89POXLksFsY7gRXSkxM1LZtu1T3qX8vP2ixWFT3qdratCltJuilR5563tKNzkSL5o3UsNHzOnbspOk4buHJ7e2JaO9ULBZZfH1v/NvP/8b/W//nw6XVKnll3Gmoq1evV5Uq9VW9eqOUZcuWnZoz5ydVr97I4zsTcJzRCkWBAgW0d+9eFS1aVH///beSk5O1d+9eVahQQZK0Z88eBQUFmYx4TwIDA1SqVHDK4+DiRVWpUgVFR1/UyZN37hBlZJ563pI09rNvNHXKWG3dtkvh4dv1do+uCgzMqmnT55qO5lKeeN7jx49Qu+dbqvVznXX5cpzy588nSYqNvazr168bTudantjevK5lrva2+GeVd+F/r/zoXbCgfEqWkvXyJVkvXVK2Di8pfv16JUdfkFfOnApo+ay88+XV9TW/S5IS9+yRLe6ycgwYoCszpqcMefIuWFAJmzYaOqv7Fxd3RXv3HrRbd/XqVV24cPGm9cC9MNqhaN++vV5++WW1aNFCYWFh6t+/v/r27asLFy7IYrFoxIgReu6550xGvCdVq1ZS2MoFKY/HjBksSZoxY546d+ltKJXreep5S9L8+b8oX97cGvxhXxUokE87d+5R02c6KDIy6u4/nIF54nm/8XpHSbL7XZekzl16a+bM+SYiuY0ntjeva5mrvX3KlFHucZ+lPM7erbsk6drS33Tp00/lXaSocg5pKK+cOWW9dEmJB/Yr+u23lXzsmKQbV3m62L+/snXpogc+GSv5+Cjp2DHFvD9QSYcPmzgluAmTsh1j9D4UVqtVI0eO1MaNG1WzZk299957mjt3rvr376+rV6+qWbNm+vzzzxUYGOjQfk3ehwLux5+8ZzF9HwpTTNyHIj3wzNb23Nc1V96HIj0zdR8K09LzfSiqF3rS2LE3n15j7NjO4sZ2yPAy3S8w7ogOhWfxzNb23Nc1OhSeJT13KB4r9ISxY4ef/sPYsZ2VcWcUAQAAADCODgUAAAAApxm/UzYAAACQnmTCGQEuRYUCAAAAgNOoUAAAAACpcNlYx1ChAAAAAOA0KhQAAABAKsyhcAwVCgAAAABOo0MBAAAAwGkMeQIAAABSYVK2Y6hQAAAAAHAaFQoAAAAgFRsVCodQoQAAAADgNDoUAAAAAJzGkCcAAAAgFSv3oXAIFQoAAAAATqNCAQAAAKTCpGzHUKEAAAAA4DQqFAAAAEAqzKFwDBUKAAAAAE6jQwEAAADAaQx5AgAAAFJhUrZjqFAAAAAAcBoVCgAAACAVJmU7JlN2KDz1V8DXO1M2510lJSeZjmCExWIxHQFu5Kmt7bG/5x76YebhbWdNRzAismM50xGA+8KQJwAAAABO88yvtAEAAIDbYFK2Y6hQAAAAAHAaFQoAAAAgFSZlO4YKBQAAAACnUaEAAAAAUmEOhWOoUAAAAABwGh0KAAAAAE5jyBMAAACQis1mNR0hQ6FCAQAAAMBpVCgAAACAVKxMynYIFQoAAAAATqNDAQAAAMBpDHkCAAAAUrFxp2yHUKEAAAAA4DQqFAAAAEAqTMp2DBUKAAAAAE6jQgEAAACkwhwKx1ChAAAAAOA0OhQAAAAAnMaQJwAAACAVK0OeHEKFAgAAAIDTqFAAAAAAqdi4bKxDqFAAAAAAcBodijT05hsddejgJsVdOqwN6xbpsWqVTUdyqYEDe+nateN2y44dYaZjuVzt2jW0cOE0HT+2VYkJEWrevKHpSG7Rv183bVi/WBei9uvUyR1aMH+ySpcuYTqWy3nqefN7TntnRiE1q+m7ORP11/61Oh97QI2b1kt5zsfHRx8M6as1G37RsdPb9df+tfp80sfKXyDIYGLneD/0sLJ2G6JsH89Wjq+WyadSiN3z/h3fUY6vltktAW+PsNsm24jpN22TpWFbd54GMhA6FGmkTZvmGjN6kIYN/1SP1Wiknbv26tcls5QvXx7T0Vxqz54DKl68WspSr95zpiO5XGBggHbt2qu3ew40HcWt6jwRoomTpqtOneZq0uQF+fj6asni2QoIyGo6mkt56nnze057Z0YBAQHas/uA3u075Kbnsgb4q2Kl8vp09ETVe6KVXunQXaUeCtZ3cyYaSHp/LFn8ZT11RNe///y22yTtDtflfu1SlquTQ2/a5vrP0+22SVj9sytjpys2m83YkhExhyKN9O7ZVZOnzNb0GfMkSW91e09NGtdTp1faadToLwync52kpCSdO3fedAy3WrZstZYtW206hts1a9bB7nGXLr11OmKXqlSpqHXr/jSUyvU89bz5Pb+B9s5cwlb+obCVf9zyucuX4tSm5at2697rN0wrVi9Q4QcLKuLUGXdETBNJe7Yoac+WO25jS0qU7dLFO+8o/trdtwFEhyJN+Pr6qkqViho56t9vAmw2m8JWrdPjj1c1mMz1SpUK1pEjm3X9erz+/HObPvzwY508edp0LLhBzpw5JEkXo2PMBnEzTz1vT0V7e7YcObLJarUqNvaS6Shpzqd0RWUbPVe2q5eVfGCn4n+eJtuVy3bbZGnYVlmavChbdKQSw1crYeWPktVqKLF7WZmU7RCjHYozZ85o4sSJWrdunc6cOSMvLy+VKFFCLVu21CuvvCJvb2+T8e5Z3ry55ePjo8hzUXbrIyPPq2yZkoZSuV54+A699to7OnjwiAoUCNLAgb20cuV8Va3aQHFxV0zHgwtZLBaNGTNY69dv1p69B0zHcRtPPW9PRXt7Nj+/LPpwSF/9uGCJ4i5nrve0pD1blLR9vaxRZ+WVr6D8WnZSQI8RuvJxL8l2o8OQsPpnJZ84JNuVy/IuWV7+LTvJkjO34ud/bTY80iVjHYotW7aofv36KlWqlLJmzaq///5bL774ohISEtS3b199++23Wrp0qbJnz37H/cTHxys+Pt5unc1mk8VicWV8SFq+/PeUf+/evV/h4Tt04MB6tW79jKZPn2suGFxu/PgRqlC+jJ6q28p0FLfy1PP2VLS35/Lx8dHkaZ/JYrGoX59BpuOkuaQta1L+bT19TMkRR5V9xHR5l6mo5P07JOlGNeKfbSKOSkmJ8u/QU/ELp0pJie6O7HYZdS6DKcYmZffq1Uu9e/fWli1btHbtWk2bNk0HDx7UnDlzdOTIEV29elXvv//+XfcTGhqqnDlz2i026+W7/lxaioqKVlJSkoLy57VbHxSUT2c9aH5BbOwlHTp0VCVLFjMdBS40btxwNWlcXw0atlVERMYZU3y/PPW8PRXt7bludCbG6cEihfRci1czXXXiVmxRZ2W9HCOvfIVuu03y0QOyePvIK09+NyZDRmGsQ7Ft2za99NJLKY9ffPFFbdu2TefOndMDDzygUaNGacGCBXfdz4ABAxQbG2u3WLzuXNVIa4mJidq2bZfqPlU7ZZ3FYlHdp2pr06atbs1iUmBggIKDi+ns2UjTUeAi48YNV4vmjdSw0fM6duyk6Thu46nn7alob8/1T2eiRMlieq7FK7p4McZ0JLew5MorS2AO2WKjb7uNV5ESslmTZb0c475gyDCMDXkKCgrSmTNnVKLEjet7nzt3TklJScqR48YEuIceekjR0bf/xf6Hn5+f/Pz87NaZGO409rNvNHXKWG3dtkvh4dv1do+uCgzMqmmZeOhPaOhALVmyUidORKhQofx6//3eSk5O1rx5v5iO5lKBgQEqVSo45XFw8aKqVKmCoqMvZuoJ6ePHj1C751uq9XOddflynPLnzydJio29rOvXrxtO5zqeet78ntPembG9AwMDFFyiaMrjosUe1MOPlNXFi7E6d/a8vp0xXhUrlVf751+Xt7e3goJujDy4eDFWiYkZaJiPn79dtcErbwF5PVhCtiuXZbt6WX7PdFDStnWyXroor3wF5d+qi6znTytp740vQb1LlJN3cFklHdgp2/Wr8i5RTv5t3lDin6ukq3GmzsqtrAx5cojFZmiQWK9evRQWFqbRo0fLz89Pw4YNk81m0+rVNy5bt2zZMnXr1k2HDh1yeN8+WQqnddx78tabr+idPm+qQIF82rlzj3r1/lCbw7e77fi+3u7tH86YMUG1a9dQ7ty5FBUVrQ0bwjVo0GgdPXrCrTmSkpPcerwnnghR2Mqbq2czZsxT5y693ZbD3R3nhPhTt1zfuUtvzZw5361Z3Cm9nLe7X6r5PbdHe7tHrqzZXLr/mrWr6+clM29aP2fWjxo18nNt+2vVLX+uRdOXtGHdZpflOtyueJruz7t0RQW+M/qm9Qkbluv67AkKeHOQvIqUkiUgULaYC0rat03xP0+X7f+rD15FSsn/xe7yLlBE8vGVNeqsEv8MuzGvIg3nT+T4alma7Sut5c7+kLFjR1/+29ixnWWsQxEXF6fOnTvrxx9/VHJyskJCQvTdd98pOPjGNyTLly9XbGys2rRp4/C+TXUoTHN3hyK9cHeHIr3gwgOexVMnCHrq77mntrerOxTpVVp3KDKK9NyheCBbKWPHvhjn+Jfpphn7BJotWzbNnTtX169fV1JSkrJls38RadCggaFkAAAAAO6V8a+0/f39TUcAAAAA4CTjHQoAAAAgPeFO2Y4xdtlYAAAAABkfFQoAAAAgFU+9MIKzqFAAAAAAcBoVCgAAACAVbmznGCoUAAAAAJxGhwIAAACA0xjyBAAAAKRi47KxDqFCAQAAAMBpVCgAAACAVJiU7RgqFAAAAACcRocCAAAAgNMY8gQAAACkwp2yHUOFAgAAAIDTqFAAAAAAqXDZWMdQoQAAAADgNDoUAAAAAJzGkCcAAAAgFSZlO4YKBQAAAACn0aEAAAAAUrHZbMYWR33xxRcqXry4/P39VaNGDW3evNkF/0XujA4FAAAAkAHNnTtXffr00aBBg7Rt2zZVqlRJDRs2VGRkpFtz0KEAAAAAUrEZXBzx6aefqmvXrurUqZPKly+vSZMmKSAgQN9++62TZ+4cOhQAAABABpOQkKCtW7eqfv36Keu8vLxUv359bdy40a1ZuMoTAAAAkE7Ex8crPj7ebp2fn5/8/Pzs1kVFRSk5OVn58+e3W58/f37t37/f5Tnt2JBmrl+/bhs0aJDt+vXrpqO4FefNeXsCzpvz9gScN+cN8wYNGnTTSKhBgwbdtF1ERIRNkm3Dhg126/v162erXr26m9LeYLHZuNBuWrl06ZJy5syp2NhY5ciRw3Qct+G8OW9PwHlz3p6A8+a8Yd69VigSEhIUEBCgBQsWqGXLlinrO3bsqJiYGP3888/uiCuJORQAAABAuuHn56ccOXLYLf/bmZCkLFmyqGrVqgoLC0tZZ7VaFRYWppCQEHdGZg4FAAAAkBH16dNHHTt2VLVq1VS9enWNGzdOV65cUadOndyagw4FAAAAkAE9//zzOn/+vD788EOdPXtWlStX1tKlS2+aqO1qdCjSkJ+fnwYNGnTLslRmxnlz3p6A8+a8PQHnzXkj4+nevbu6d+9uNAOTsgEAAAA4jUnZAAAAAJxGhwIAAACA0+hQAAAAAHAaHQoAAAAATqNDkYa++OILFS9eXP7+/qpRo4Y2b95sOpJL/fHHH2rWrJkKFSoki8Win376yXQktwgNDdVjjz2m7NmzKygoSC1bttSBAwdMx3K5iRMnqmLFiik32QkJCdFvv/1mOpbbjRw5UhaLRb169TIdxaUGDx4si8Vit5QtW9Z0LLeIiIhQhw4dlCdPHmXNmlWPPPKItmzZYjqWSxUvXvym9rZYLOrWrZvpaC6VnJysDz74QMHBwcqaNatKliypYcOGyROuV3P58mX16tVLxYoVU9asWVWzZk2Fh4ebjoUMig5FGpk7d6769OmjQYMGadu2bapUqZIaNmyoyMhI09Fc5sqVK6pUqZK++OIL01Hcas2aNerWrZs2bdqkFStWKDExUQ0aNNCVK1dMR3OpBx98UCNHjtTWrVu1ZcsW1a1bVy1atNCePXtMR3Ob8PBwffXVV6pYsaLpKG5RoUIFnTlzJmVZt26d6Ugud/HiRdWqVUu+vr767bfftHfvXn3yySd64IEHTEdzqfDwcLu2XrFihSSpTZs2hpO51scff6yJEyfq888/1759+/Txxx9r1KhRmjBhguloLtelSxetWLFCM2fO1F9//aUGDRqofv36ioiIMB0NGZENaaJ69eq2bt26pTxOTk62FSpUyBYaGmowlftIsi1cuNB0DCMiIyNtkmxr1qwxHcXtHnjgAdvkyZNNx3CLy5cv2x566CHbihUrbE8++aStZ8+epiO51KBBg2yVKlUyHcPt3n33XVvt2rVNxzCuZ8+etpIlS9qsVqvpKC7VtGlT26uvvmq3rlWrVrb27dsbSuQeV69etXl7e9sWL15st75KlSq2gQMHGkqFjIwKRRpISEjQ1q1bVb9+/ZR1Xl5eql+/vjZu3GgwGdwhNjZWkpQ7d27DSdwnOTlZc+bM0ZUrVxQSEmI6jlt069ZNTZs2tfs7z+z+/vtvFSpUSCVKlFD79u114sQJ05Fc7pdfflG1atXUpk0bBQUF6dFHH9U333xjOpZbJSQk6LvvvtOrr74qi8ViOo5L1axZU2FhYTp48KAkaefOnVq3bp0aN25sOJlrJSUlKTk5Wf7+/nbrs2bN6hGVSKQ97pSdBqKiopScnHzTbc7z58+v/fv3G0oFd7BarerVq5dq1aqlhx9+2HQcl/vrr78UEhKi69evK1u2bFq4cKHKly9vOpbLzZkzR9u2bfOo8cU1atTQtGnTVKZMGZ05c0ZDhgxRnTp1tHv3bmXPnt10PJc5cuSIJk6cqD59+ui///2vwsPD9fbbbytLlizq2LGj6Xhu8dNPPykmJkavvPKK6Sgu99577+nSpUsqW7asvL29lZycrBEjRqh9+/amo7lU9uzZFRISomHDhqlcuXLKnz+/vv/+e23cuFGlSpUyHQ8ZEB0K4D5069ZNu3fv9phvdMqUKaMdO3YoNjZWCxYsUMeOHbVmzZpM3ak4efKkevbsqRUrVtz0bV5mlvob2ooVK6pGjRoqVqyY5s2bp86dOxtM5lpWq1XVqlXTRx99JEl69NFHtXv3bk2aNMljOhRTpkxR48aNVahQIdNRXG7evHmaNWuWZs+erQoVKmjHjh3q1auXChUqlOnbe+bMmXr11VdVuHBheXt7q0qVKnrhhRe0detW09GQAdGhSAN58+aVt7e3zp07Z7f+3LlzKlCggKFUcLXu3btr8eLF+uOPP/Tggw+ajuMWWbJkSfn2qmrVqgoPD9dnn32mr776ynAy19m6dasiIyNVpUqVlHXJycn6448/9Pnnnys+Pl7e3t4GE7pHrly5VLp0aR06dMh0FJcqWLDgTR3kcuXK6YcffjCUyL2OHz+ulStX6scffzQdxS369eun9957T+3atZMkPfLIIzp+/LhCQ0MzfYeiZMmSWrNmja5cuaJLly6pYMGCev7551WiRAnT0ZABMYciDWTJkkVVq1ZVWFhYyjqr1aqwsDCPGV/uSWw2m7p3766FCxdq1apVCg4ONh3JGKvVqvj4eNMxXKpevXr666+/tGPHjpSlWrVqat++vXbs2OERnQlJiouL0+HDh1WwYEHTUVyqVq1aN10G+uDBgypWrJihRO41depUBQUFqWnTpqajuMXVq1fl5WX/Ucjb21tWq9VQIvcLDAxUwYIFdfHiRS1btkwtWrQwHQkZEBWKNNKnTx917NhR1apVU/Xq1TVu3DhduXJFnTp1Mh3NZeLi4uy+rTx69Kh27Nih3Llzq2jRogaTuVa3bt00e/Zs/fzzz8qePbvOnj0rScqZM6eyZs1qOJ3rDBgwQI0bN1bRokV1+fJlzZ49W7///ruWLVtmOppLZc+e/ab5MYGBgcqTJ0+mnjfTt29fNWvWTMWKFdPp06c1aNAgeXt764UXXjAdzaV69+6tmjVr6qOPPlLbtm21efNmff311/r6669NR3M5q9WqqVOnqmPHjvLx8YyPB82aNdOIESNUtGhRVahQQdu3b9enn36qV1991XQ0l1u2bJlsNpvKlCmjQ4cOqV+/fipbtmym/twCFzJ9manMZMKECbaiRYvasmTJYqtevbpt06ZNpiO51OrVq22Sblo6duxoOppL3eqcJdmmTp1qOppLvfrqq7ZixYrZsmTJYsuXL5+tXr16tuXLl5uOZYQnXDb2+eeftxUsWNCWJUsWW+HChW3PP/+87dChQ6ZjucWiRYtsDz/8sM3Pz89WtmxZ29dff206klssW7bMJsl24MAB01Hc5tKlS7aePXvaihYtavP397eVKFHCNnDgQFt8fLzpaC43d+5cW4kSJWxZsmSxFShQwNatWzdbTEyM6VjIoCw2mwfcDhIAAACASzCHAgAAAIDT6FAAAAAAcBodCgAAAABOo0MBAAAAwGl0KAAAAAA4jQ4FAAAAAKfRoQAAAADgNDoUAOCkV155RS1btkx5/J///Ee9evVye47ff/9dFotFMTExt93GYrHop59+uud9Dh48WJUrV76vXMeOHdP/tXe3MVWXfxzH30fo4AEPmonoMUQLxOOG5M3WcFNGqZCbkujYFO1YREtwOm+RNVREo1X4AJs3SwXnzdBpYCLOMecNG9ZW3uRKqXNC0emDcs5GBYLn+j9onn9HtOD8/S+lz+vZ77q+5/p+fzw4O99zXb+DxWLh/Pnz/9M6IiLyZFNDISLdyrx587BYLFgsFqxWKzExMaxdu5b29vb/e+7PPvuMoqKiTsV2pgkQERF5GgT/0wWIiDxuqamplJWV0draSk1NDbm5uTzzzDPk5+d3iL179y5Wq/Wx5O3bt+9jWUdERORpoh0KEel2QkJCGDBgANHR0cyfP5+JEyfy+eefA/89prR+/XocDgdxcXEAXLt2jYyMDPr06UPfvn1JS0vjypUrvjXv3bvHkiVL6NOnD8899xwrVqzAGOOX98EjT62treTl5REVFUVISAgxMTFs376dK1eukJycDMCzzz6LxWJh3rx5AHi9XoqLixk6dCg2m42EhAQOHDjgl6empoZhw4Zhs9lITk72q7Oz8vLyGDZsGKGhobzwwgsUFBTQ1tbWIW7r1q1ERUURGhpKRkYGd+7c8Zvftm0bTqeTnj17Mnz4cDZt2vTInLdv3yYzM5OIiAhsNhuxsbGUlZV1uXYREXmyaIdCRLo9m83GrVu3fNfHjx8nPDyc2tpaANra2khJSSExMZG6ujqCg4NZt24dqampfPPNN1itVkpKSigvL2fHjh04nU5KSkqorKzklVdeeWTeN954gzNnzlBaWkpCQgKNjY38/PPPREVFcfDgQWbMmEFDQwPh4eHYbDYAiouL2b17N1u2bCE2NpbTp08zZ84cIiIiSEpK4tq1a6Snp5Obm8s777zDV199xdKlS7v8N7Hb7ZSXl+NwOLh48SLZ2dnY7XZWrFjhi3G73ezfv5/Dhw/zyy+/kJWVRU5ODnv27AFgz549rFq1ik8++YRRo0Zx7tw5srOzCQsLw+VydchZUFDAd999x9GjR+nXrx9ut5vff/+9y7WLiMgTxoiIdCMul8ukpaUZY4zxer2mtrbWhISEmGXLlvnmIyMjTWtrq+81u3btMnFxccbr9frGWltbjc1mM8eOHTPGGDNw4EDz4Ycf+ubb2trM888/78tljDFJSUlm0aJFxhhjGhoaDGBqa2sfWueJEycMYG7fvu0ba2lpMaGhoaa+vt4vNisry8yaNcsYY0x+fr4ZMWKE33xeXl6HtR4EmMrKykfOf/TRR2bMmDG+69WrV5ugoCBz/fp139jRo0dNjx49zM2bN40xxrz44otm7969fusUFRWZxMREY4wxjY2NBjDnzp0zxhgzdepU8+abbz6yBhEReTpph0JEup3q6mp69epFW1sbXq+X2bNns2bNGt98fHy833MTFy5cwO12Y7fb/dZpaWnB4/Fw584dbt68ycsvv+ybCw4OZuzYsR2OPd13/vx5goKCSEpK6nTdbreb3377jUmTJvmN3717l1GjRgFw6dIlvzoAEhMTO53jvn379lFaWorH46G5uZn29nbCw8P9YgYPHsygQYP88ni9XhoaGrDb7Xg8HrKyssjOzvbFtLe307t374fmnD9/PjNmzODs2bNMnjyZ119/nXHjxnW5dhERebKooRCRbic5OZnNmzdjtVpxOBwEB/u/1YWFhfldNzc3M2bMGN9Rnj+LiIgIqIb7R5i6orm5GYAjR474fZCHP54LeVzOnDlDZmYmhYWFpKSk0Lt3byoqKigpKelyrZ9++mmHBicoKOihr3nttde4evUqNTU11NbW8uqrr5Kbm8vHH38c+M2IiMg/Tg2FiHQ7YWFhxMTEdDp+9OjR7Nu3j/79+3f4lv6+gQMH8uWXXzJhwgTgj2/iv/76a0aPHv3Q+Pj4eLxeL6dOnWLixIkd5u/vkNy7d883NmLECEJCQmhqanrkzobT6fQ9YH7fF1988fc3+Sf19fVER0fz3nvv+cauXr3aIa6pqYkbN27gcDh8eXr06EFcXByRkZE4HA5+/PFHMjMzO507IiICl8uFy+Vi/PjxLF++XA2FiMhTTr/yJCL/epmZmfTr14+0tDTq6upobGzk5MmTLFy4kOvXrwOwaNEiPvjgA6qqqrh8+TI5OTl/+T8khgwZgsvl4q233qKqqsq35v79+wGIjo7GYrFQXV3NTz/9RHNzM3a7nWXLlrF48WJ27tyJx+Ph7NmzbNy4kZ07dwLw7rvv8sMPP7B8+XIaGhrYu3cv5eXlXbrf2NhYmpqaqKiowOPxUFpaSmVlZYe4nj174nK5uHDhAnV1dSxcuJCMjAwGDBgAQGFhIcXFxZSWlvL9999z8eJFysrK2LBhw0Pzrlq1ikOHDuF2u/n222+prq7G6XR2qXYREXnyqKEQkX+90NBQTp8+zeDBg0lPT8fpdJKVlUVLS4tvx2Lp0qXMnTsXl8tFYmIidrud6dOn/+W6mzdvZubMmeTk5DB8+HCys7P59ddfARg0aBCFhYWsXLmSyMhIFixYAEBRUREFBQUUFxfjdDpJTU3lyJEjDB06FPjjuYaDBw9SVVVFQkICW7Zs4f333+/S/U6bNo3FixezYMECXnrpJerr6ykoKOgQFxMTQ3p6OlOmTGHy5MmMHDnS72dh3377bbZt20ZZWRnx8fEkJSVRXl7uq/VBVquV/Px8Ro4cyYQJEwgKCqKioqJLtYuIyJPHYh71RKGIiIiIiMjf0A6FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgETA2FiIiIiIgE7D/zHS2iAdTIMAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(val_y_true, val_y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "UG11RIE5-Ayx",
        "outputId": "5f483f8a-857d-4486-d647-5b3bc1a7f6d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c7b311b5-9a11-4e2d-925d-59c23ddfae90\" class=\"plotly-graph-div\" style=\"height:400px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c7b311b5-9a11-4e2d-925d-59c23ddfae90\")) {                    Plotly.newPlot(                        \"c7b311b5-9a11-4e2d-925d-59c23ddfae90\",                        [{\"mode\":\"lines\",\"name\":\"Training Accuracy\",\"y\":[0.13602403849363326,0.20661538463830947,0.27491346156597135,0.336043269276619,0.3742788462638855,0.40582211542129515,0.43963461542129517,0.45597596156597137,0.46400961542129515,0.48101442313194276,0.4888942308425903,0.49194711542129516,0.5013701922893524,0.5066057692766189,0.5209663462638855,0.5271971154212952,0.5301105771064758,0.54475,0.5604519231319427,0.561336538553238,0.5821826922893524,0.5929855771064758,0.6049326922893524,0.6148173077106476,0.6157163462638855,0.6339615385532379,0.6345528848171235,0.6480192308425903,0.6507403848171234,0.6428125,0.658899038553238,0.6614326922893524,0.6726442308425903,0.6688076922893524,0.6728365385532379,0.671524038553238,0.6815865385532379,0.6733990385532379,0.6855192308425904,0.6868942308425904,0.6890721154212952,0.6834807693958282,0.6873221154212952,0.6970144231319427,0.6977067308425904,0.7007067308425904,0.7008076922893525,0.7016730771064759,0.7028990385532379,0.7117932693958282,0.7090721154212951,0.7073605771064758,0.7087644231319428,0.7101442308425904,0.7206394231319427,0.7142355771064758,0.7138076922893524,0.7191730771064758,0.7233557693958282,0.7237451922893524,0.7213028848171235,0.7252596154212951,0.7207403848171234,0.7245240385532379,0.7303894231319428,0.7297980771064758,0.7327644231319428,0.7308990385532379,0.7300769231319427,0.7325192308425903,0.7334230771064758,0.7363365385532379,0.7410096154212952,0.7383269231319427,0.7405288462638855,0.7439134616851807,0.7412355771064758,0.7392067308425904,0.7421730771064758,0.7436730771064758,0.7407355771064759,0.7448942308425903,0.7506682693958282,0.7471682693958283,0.7432451922893524,0.7453557693958283,0.7487067308425903,0.7533846154212952,0.7524855771064758,0.752221153974533,0.7502644231319427,0.7573221154212951,0.7481682693958283,0.7542067308425904,0.7587980771064758,0.7542019231319428,0.7549855771064758,0.7599423079490661,0.758024038553238,0.7581682693958283,0.7600432693958282,0.7558557693958282,0.7645865385532379,0.7613653848171235,0.7636730771064758,0.7618942308425903,0.7624855771064758,0.7677403848171234,0.7649230771064758,0.7632307693958282,0.7649134616851807,0.7693028848171234,0.7669567308425903,0.7695192308425903,0.7668653848171234,0.7706105771064758,0.7663317308425903,0.7664567308425904,0.7692307693958282,0.7734807693958282,0.7690432693958282,0.7683317308425903,0.7736730771064758,0.7708942308425903,0.7670576922893524,0.7760769231319428,0.7755096154212952,0.7717355771064758,0.7772644231319428,0.7725384616851807],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Accuracy\",\"y\":[0.17216614899890764,0.2733889754329409,0.30427503905126024,0.42223893638168064,0.44654988391058786,0.40093653010470526,0.5431871116161346,0.4703270580087389,0.553353066955294,0.5410762812410083,0.5270283392497471,0.5423136651515961,0.5348408392497471,0.4333996506673949,0.5747525244951248,0.5578173526695797,0.5456376160894122,0.5495438660894122,0.4899796194263867,0.5788043481963021,0.566066575901849,0.5949631205626896,0.6558860646826881,0.6434879664863858,0.675077640584537,0.6606900244951248,0.6724087744951248,0.5792168102094105,0.7210549307721001,0.7064489530665534,0.7080987968615123,0.7226077254329409,0.6576814843075616,0.6695215446608407,0.7491750781025205,0.7178280289684024,0.7096758548702512,0.7123689843075616,0.7206182075398309,0.7054299307721001,0.7038771361112595,0.7565508548702512,0.762349573629243,0.7265382387808391,0.7325553191559655,0.7161539218255452,0.7122476718255452,0.7462878503969738,0.6816770200218473,0.763344332575798,0.744807841522353,0.7611121897186551,0.7543187120131084,0.7543187120131084,0.773728649531092,0.7412655289684024,0.7664741852453777,0.738912073629243,0.7787509709596634,0.7602387432541166,0.7618885870490756,0.7256405289684024,0.7559928191559655,0.7853260870490756,0.7198418102094105,0.7653581138168063,0.766255823629243,0.7761791540043694,0.7726125781025205,0.7483986807721001,0.772733890584537,0.7303231762988227,0.7470642477273941,0.762349573629243,0.7760821048702512,0.7993982923882348,0.7402465066739491,0.7878008548702512,0.7763975156205041,0.7353454977273941,0.7747476718255452,0.7529842789684024,0.784452640584537,0.7660374620131084,0.7849864129509244,0.7872185558080673,0.7937208861112595,0.7874611807721001,0.7572302030665534,0.7918041540043694,0.7998350156205041,0.7692643638168063,0.7856657611472266,0.8019701093435287,0.7537606762988227,0.7534210022006717,0.7812985245670591,0.7825601718255452,0.7671535334416798,0.7962684397186551,0.8058763593435287,0.7943759709596634,0.775063082575798,0.7837732923882348,0.7816624620131084,0.8061917700937816,0.7992769799062184,0.7880192164863858,0.8038625781025205,0.802067158477647,0.7880192164863858,0.7807404888527734,0.781541149531092,0.7985248459236962,0.8131065602813449,0.7926048146826881,0.8035229040043694,0.7978454977273941,0.8045419262988227,0.7899116852453777,0.8056579977273941,0.8121118013347898,0.7884559397186551,0.8030861807721001,0.8042993013347898,0.782875582575798,0.8026251941919327,0.797505823629243,0.8130095111472266,0.8039596272366387],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Training and Validation Loss over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"width\":900,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c7b311b5-9a11-4e2d-925d-59c23ddfae90');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objects as graph\n",
        "# pio.renderers\n",
        "\n",
        "fig = graph.Figure()\n",
        "fig.add_trace(graph.Scatter(y=train_acc, mode='lines', name='Training Accuracy'))\n",
        "fig.add_trace(graph.Scatter(y=val_acc, mode='lines', name='Validation Accuracy'))\n",
        "fig.update_layout(title='Training and Validation Loss over Epochs',\n",
        "                  xaxis_title='Epoch',\n",
        "                  yaxis_title='Loss',\n",
        "                  width=900,\n",
        "                  height=400)\n",
        "\n",
        "# fig.add_trace(go.Scatter(y=val_losses, fill='tozeroy', mode='lines', name='Validation Loss'))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "XNdlb6-sJp6z",
        "outputId": "ac558366-651a-4726-a430-8e7d57363b00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"961623aa-9462-4f87-8348-891643f3a39f\" class=\"plotly-graph-div\" style=\"height:400px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"961623aa-9462-4f87-8348-891643f3a39f\")) {                    Plotly.newPlot(                        \"961623aa-9462-4f87-8348-891643f3a39f\",                        [{\"mode\":\"lines\",\"name\":\"Training Loss\",\"y\":[2.2592493181228637,2.089831184387207,1.924033341884613,1.7459484090805053,1.6602383460998535,1.5914791464805602,1.5178361477851867,1.486564564704895,1.4640762424468994,1.421599790096283,1.4130968289375305,1.3924338717460631,1.363593867778778,1.34555064868927,1.3267663578987121,1.3190142126083375,1.306860605955124,1.2915702242851257,1.2612815670967101,1.2569673013687135,1.2159947793483734,1.18675026845932,1.1747621636390686,1.1442191896438598,1.1378074781894685,1.1060402610301971,1.0926968855857848,1.0663561086654663,1.05315900182724,1.0729797220230102,1.0444344108104706,1.023303567647934,1.0118242197036744,1.0098660626411438,1.0010481193065643,1.0071480088233948,0.9838061878681182,1.0002789096832276,0.9815294713973999,0.9677466654777527,0.9690529718399048,0.9728074336051941,0.9594841611385345,0.949697936296463,0.949204822063446,0.9391341753005982,0.9388946297168732,0.9398263566493988,0.9481476955413818,0.9269249868392945,0.9238190560340881,0.9264741690158844,0.919983181476593,0.9257200820446014,0.9008940906524658,0.9051415741443634,0.9027090764045715,0.893316034078598,0.8912543549537658,0.8857265882492066,0.8929676423072815,0.8852191727161407,0.8877710418701172,0.8798793246746063,0.8655754990577698,0.8632766752243042,0.8676694960594177,0.8590169711112976,0.8625742554664612,0.8529208989143372,0.8494498336315155,0.8406931006908417,0.8339783024787902,0.8467954597473144,0.8434980673789978,0.8272889616489411,0.8273126618862152,0.8412406082153321,0.8290133786201477,0.8178417325019837,0.8229347908496857,0.8222632925510407,0.8087147235870361,0.8167674188613891,0.8143112785816192,0.8167440259456634,0.8150745093822479,0.7963912487030029,0.7955697138309479,0.7954758132696151,0.8035228190422058,0.7972115886211395,0.7997210566997528,0.7835767431259155,0.7761113927364349,0.7814811238050461,0.7813908129930496,0.7804935219287872,0.7800469956398011,0.7775717353820801,0.7720194361209869,0.786787502527237,0.7616498129367828,0.7695389904975891,0.7685364446640015,0.7701365453004837,0.7712788894176483,0.7492162034511566,0.7656741173267364,0.7686733117103577,0.7658600580692291,0.7427341556549072,0.7600354504585266,0.7505570569038391,0.755768553853035,0.756342618227005,0.7499987514019012,0.7576923034191132,0.7530001907348632,0.7345001217126846,0.7466406388282776,0.7416050432920456,0.7360805002450943,0.7441189317703247,0.745975679397583,0.7360852406024933,0.732792789697647,0.7396623842716217,0.7220872031450272,0.7369037024974823],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation Loss\",\"y\":[2.2082246201378957,1.9170317990439278,1.8223817944526672,1.5215899518557958,1.4476728098733085,1.5046277301652091,1.2874781617096491,1.5172413204397475,1.2379112371376582,1.2460744636399406,1.308188693864005,1.203184310879026,1.2559482625552587,1.5204227438994817,1.1999728892530714,1.2247390619346075,1.2597141308443887,1.1863544114998408,1.480194785765239,1.106053712112563,1.253888773066657,1.125632888504437,1.0313669017383031,1.0491499177047185,0.9534841435296195,1.0381210063185011,0.998866794364793,1.2006995933396476,0.8318154811859131,0.8814687750169209,0.8886363293443408,0.8515137199844632,1.0064793335539954,1.0289362711565835,0.8022617548704147,0.8403652608394623,0.9051441954714912,0.8902305705206734,0.8454147066388812,0.9073167485850198,0.8850665986537933,0.7753520820822034,0.7600130481379372,0.8308069982698986,0.8466949484177998,0.8973547560828072,0.866791137627193,0.7873481959104538,0.9567845229591642,0.764483283673014,0.8404295636074883,0.7556925181831632,0.7497952261141368,0.7597991174885205,0.6928313161645617,0.7825773230620793,0.7673646892820086,0.8087606749364308,0.7407615951129368,0.7673327199050358,0.7488985934427806,0.8372126775128501,0.7812248564192227,0.6982987714665276,0.8508846908807755,0.7539000170571464,0.7235115574938911,0.7028253142322812,0.7277358642646244,0.7554170648966517,0.7028142724718366,0.8325911866767066,0.8169257204447474,0.7793573013373783,0.7445344775915146,0.6496159583330154,0.7970884293317795,0.66524417166199,0.6920357452971595,0.8258371757609504,0.7329201943108014,0.8109808681266648,0.6608338451811245,0.7404604256153107,0.6891630270651409,0.6647098852055413,0.6671841868332454,0.6783519036003521,0.753138827426093,0.6572164797357151,0.6286464237741062,0.7121326497622898,0.6888756028243473,0.6362558752298355,0.7413639851978847,0.7811401124511447,0.7008217221924237,0.6920595669320652,0.7292825813804354,0.6563424680914197,0.6547989259873118,0.6532470913869994,0.6954510563186237,0.687544339469501,0.7015859484672546,0.6180004094328199,0.642915312732969,0.6862783176558358,0.6183929134692464,0.6384108513593674,0.643685733633382,0.690689680831773,0.6730626906667437,0.6242386581642287,0.6044715430055346,0.6555882170796394,0.6079148011548179,0.6633256333214896,0.6346899026206562,0.6544495533619609,0.6075204419238227,0.6173043102025986,0.6535850869757789,0.6122255580765861,0.6213848867586681,0.6738885885902813,0.6144819366080421,0.6442462344254766,0.611494854092598,0.637941908623491],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Training and Validation Loss over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"width\":900,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('961623aa-9462-4f87-8348-891643f3a39f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.graph_objects as graph\n",
        "# pio.renderers\n",
        "\n",
        "fig = graph.Figure()\n",
        "fig.add_trace(graph.Scatter(y=train_loss, mode='lines', name='Training Loss'))\n",
        "fig.add_trace(graph.Scatter(y=val_loss, mode='lines', name='Validation Loss'))\n",
        "fig.update_layout(title='Training and Validation Loss over Epochs',\n",
        "                  xaxis_title='Epoch',\n",
        "                  yaxis_title='Loss',\n",
        "                  width=900,\n",
        "                  height=400)\n",
        "\n",
        "# fig.add_trace(go.Scatter(y=train_losses, fill='tozeroy', mode='lines', name='Training Loss'))  # Fill to x-axis\n",
        "# fig.add_trace(go.Scatter(y=val_losses, fill='tozeroy', mode='lines', name='Validation Loss'))\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}